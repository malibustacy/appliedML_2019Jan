---
title: "Prediction"
subtitle: "With regression, decision trees, and random forests"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
output:
  html_document:
    css: practical.css
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)
```

```{r, echo = FALSE, fig.align = 'center', eval = TRUE, out.width = "100%", fig.cap="Source: Medium.com"}
knitr::include_graphics("wrongdata.gif")
```

## {.tabset}

### Overview

By the end of this practical you will:

1. Understand the importance the curse of dimensionality.
2. Know how to preprocess data. 
4. Explore new features.

Ideas...

show misprediction as data grows large
show time taken as data grows large
reduce dimensionality
variable importance
do scaling with lasso and ridge
Z explore new features


### Datasets

```{r, eval = TRUE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
```

|File  |Rows | Columns |
|:----|:-----|:------|
|[pima_diabetes](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/pima_diabetes.csv)| 724 | 7|
|[violent_crime](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/violent_crime.csv)| 1000 | 102|
|[nonviolent_crime](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/nonviolent_crime.csv)| 1000 | 102|


```{r, message = FALSE, eval = TRUE, echo = FALSE}
# Load datasets locally
library(tidyverse)
pima_diabetes <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/pima_diabetes.csv")
violent_crime <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/violent_crime.csv")
nonviolent_crime <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/nonviolent_crime.csv")
```

- The `pima_diabetes` is a subset of the `PimaIndiansDiabetes2` dataset in the `mlbench` package. To see column descriptions, run the following:

```{r, eval = FALSE}
library(mlbench)       # Load ISLR package
?PimaIndiansDiabetes2  # Look at help menu for College
```

- The `violent_crime` and `non_violent_crime` data are subsets of the Communities and Crime Unnormalized Data Set dataset from the UCI Machine Learning Repository. To see column descriptions, visit this site: [Communities and Crime Unnormalized Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized)

### Glossary

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `trainControl()`|`caret`|    Define modelling control parameters| 
| `train()`|`caret`|    Train a model|
| `predict(object, newdata)`|`stats`|    Predict the criterion values of `newdata` based on `object`|
| `postResample()`|`caret`|   Calculate aggregate model performance in regression tasks|
| `confusionMatrix()`|`caret`|   Calculate aggregate model performance in classification tasks| 
| `varImp()`|`caret`| Determine the model-specific importance of features |
| `findCorrelation()`, `nearZeroVar()` |`caret`|  Identify highly correlated and low variance features. | 
| `rfe()`, `rfeControl()` |`caret`|  Run and control recursive feature selection. | 

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`caret`|`install.packages("caret")`|

### Cheatsheet

<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf">
  <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet-200x155@2x.png" alt="Trulli" style="width:70%">
  <figcaption>hhttps://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption></a>
</figure>

### Examples

```{r, eval = FALSE, echo = TRUE}

# Fitting and evaluating a regression model ------------------------------------

# Step 0: Load packages-----------

library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 

# Step 1: Load, prepare, and explore data ----------------------

# read data
data <- read_csv("1_Data/mpg_num.csv")

# Convert all characters to factors
data <- data %>%
  mutate_if(is.character, factor)

# Explore training data
data        # Print the dataset
dim(data)   # Print dimensions
names(data) # Print the names

# Step 2: Create training and test sets -------------

# Create train index
train_index <- createDataPartition(criterion, 
                                  p = .8, list = FALSE)

# Create training and test sets
data_train <- data %>% slice(train_index)
data_test <- data %>% slice(-train_index)

# split predictors and criterion
criterion_train <- data_train %>% select(hwy) %>% pull()
predictors_train <- data_train %>% select(-hwy)
criterion_test <- data_test %>% select(hwy) %>% pull()
predictors_test <- data_test %>% select(-hwy)

# Step 3: Clean data -------------

# Test for excessively correlated features
corr_matrix <- cor(predictors_train)
corr_features <- findCorrelation(corr_matrix)

# Remove excessively correlated features
predictors_train <- predictors_train %>% select(-corr_features)

# Test for near zero variance features
zerovar_features <- nearZeroVar(predictors_train)

# Remove near zero variance features
predictors_train <- predictors_train %>% select(-zerovar_features)

# recombine data
data_train <- predictors_train %>% add_column(hwy = criterion_train)

# Step 4: Define training control parameters -------------

# Train using cross-validation
train_control <- trainControl(method = "cv") 

# Step 5: Fit models -------------

# Fit glm vanilla flavor
hwy_glm <- train(form = hwy ~ .,
                 data = data_train,
                 method = "glm",
                 trControl = train_control)

# Fit with pca transformation
hwy_glm_pca <- train(form = hwy ~ .,
                     data = data_train,
                     method = "glm",
                     trControl = train_control,
                     preProcess = c('pca'))

# Fit scaling and centering
hwy_glm_sca <- train(form = hwy ~ .,
                     data = data_train,
                     method = "glm",
                     trControl = train_control,
                     preProcess = c('scale', 'center'))

# Get fits
glm_fit     <- predict(hwy_glm)
glm_pca_fit <- predict(hwy_glm_pca)
glm_sca_fit <- predict(hwy_glm_sca)

# Step 6: Evaluate variable importance -------------

# Run varImp()
imp_glm     <- varImp(hwy_glm)
imp_glm_pca <- varImp(hwy_glm_pca)
imp_glm_sca <- varImp(hwy_glm_sca)

# Plot variable importance
plot(imp_glm)
plot(imp_glm_pca)
plot(imp_glm_sca)

# Step 7: Select variables -------------

# Select by hand in formula
hwy_glm_sel <- train(form = hwy ~ cty,
                     data = data_train,
                     method = "glm",
                     trControl = train_control)

# Select by hand in data
hwy_glm_sel <- train(form = hwy ~ cty,
                     data = data_train %>% 
                       select(-cyl, -displ, -year),
                     method = "glm",
                     trControl = train_control)

# Select by reducing pca criterion ---

# Reduce criterion to 50% variance epxlained 
train_control_pca <- trainControl(method = "cv",
                                  preProcOptions = list(thresh = 0.50)) 

# Refit model with update
hwy_glm_sel <- train(form = hwy ~ cty,
                     data = data_train %>% 
                       select(-cyl, -displ, -year),
                     method = "glm",
                     trControl = train_control_pca,
                     preProcess = c('pca'))

# Step 8: Recursive feature elimination -------------

# Feature elimination settings 
rfe_control <- rfeControl(functions = lmFuncs,  # linear model
                          method = "cv",
                          verbose = FALSE)

# Run feature elimination
profile <- rfe(x = predictors_train, 
               y = criterion_train,
               sizes = c(1, 2, 3),     # Features set sizes should be considered
               rfeControl = rfe_control)

# plot result
trellis.par.set(caretTheme())
plot(profile, type = c("g", "o"))

# Step 9: Evaluate models -------------

# you know how...

```

# Tasks

## A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data file(s) listed in the `Datasets` section above are in your `1_Data` folder.

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `Features_practical.R` in the `2_Code` folder.  

3. Using `library()` load the set of packages for this practical listed in the packages section above.

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Prediction Practical

library(tidyverse)
library(caret)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(tidyverse)
```

4. In the code below, we will load each of the datasets listed in the `Datasets` as new objects.

```{r, echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE}
# Pimo Indians diabetes
pima_diabetes <- read_csv(file = "1_Data/pima_diabetes.csv")

# (Non-) violent crime statistics
violent_crime    <- read_csv(file = "1_Data/violent_crime.csv")
nonviolent_crime <- read_csv(file = "1_Data/nonviolent_crime.csv")
```


## B - Pima Indians Diabetes

In this section, you will explore feature selection for the Pima Indians Diabetes data set. The Pima are a group of Native Americans living in Arizona. A genetic predisposition allowed this group to survive normally to a diet poor of carbohydrates for years. In the recent years, because of a sudden shift from traditional agricultural crops to processed foods, together with a decline in physical activity, made them develop the highest prevalence of type 2 diabetes and for this reason they have been subject of many studies. 

1. Take a look at the first few rows of the pima diabetes data frame by printing then to the console.

2. Print the numbers of rows and columns of each dataset using the `dim()` function.

3. Look at the names of the dataframe with the `names()` function.

4. Open the dataset in a new window using `View()`. Do they look ok?

### Splitting

5. As always, before you do anything you need to make sure that you separate a hold-out data set for later. Create `pima_train` and `pima_test` using `createDataPartition()` with as little as <b>15% of cases going into the training set</b>. Also store the variable `diabetes` in the test set, which will be the criterion . 

```{r, echo = TRUE}
# split index
train_index <- createDataPartition(XX$XX, p = .15, list = FALSE)

# train and test sets
pima_train <- XX %>% slice(train_index)
pima_test  <- XX %>% slice(-train_index)

# test criterion
criterion <- pima_test$XX
```

```{r}
# split index
train_index <- createDataPartition(pima_diabetes$diabetes, p = .15, list = FALSE)

# train and test sets
pima_train <- pima_diabetes %>% slice(train_index)
pima_test  <- pima_diabetes %>% slice(-train_index)

# test criterion
criterion <- pima_test$diabetes
```

### Remove unwanted features

Ok, with the training set, let's get to work and remove some features.

6. First split the training data into a data frame holding the predictors and the criterion using the code below.

```{r, echo = TRUE}
# Select predictors
pima_train_pred <- pima_train %>% select(-diabetes)

# Select criterion
pima_train_crit <- pima_train %>% select(diabetes) %>% pull()
```

7. Although, this dataset is rather small and rather well curated, test if there are any excessively correlated features using `cor()` and `findCorrelation()` using the code below. Are there any?

```{r, echo = TRUE}
# determine correlation matrix
corr_matrix <- cor(XX_pred)

# find excessively correlated variables
findCorrelation(corr_matrix)
```

```{r}
# determine correlation matrix
corr_matrix <- cor(pima_train_pred)

# find excessively correlated variables
findCorrelation(corr_matrix)
```

8. Now, test if there are any near-zero variance predictors. Any of those?

```{r, echo = TRUE}
# find near zero variance predictors
nearZeroVar(XX_pred)
```

```{r}
# find near zero variance predictors
nearZeroVar(pima_train_pred)
```

## Feature importance

After having retained all features in the previous section section, this section explores feature selection on grounds of feature importance. To do this, we first need to fit our model. How about a simple logistic gression aka `method = 'glm'`? 

9. Fit the `glm` to the training data.

```{r, echo = TRUE}
# fit random forest
pima_glm <- train(diabetes ~ .,
                data = XX,
                method = XX
                )
```

```{r}
# fit random forest
pima_glm <- train(diabetes ~ .,
                data = pima_train,
                method = 'glm')
```

10. Evaluate the results s

```{r, echo = TRUE}
# determine variable importance
varimp_glm <- varImp(XX)

# print variable importance
varimp_glm

# print variable importance
plot(varimp_glm)
```

```{r}
# determine variable importance
varimp_glm <- varImp(pima_glm)

# print variable importance
varimp_glm

# print variable importance
plot(varimp_glm)
```


```{r, echo = TRUE}
# fit glm with best four features
pima_glm4 = train(diabetes ~ XX + YY + ZZ + AA,
                data = XX,
                method = XX)
```

```{r}
# fit glm with best four features
pima_glm4 = train(diabetes ~ glucose + mass + pregnant + pedigree,
                data = pima_train,
                method = 'glm')
```


```{r, echo = TRUE}
# fit random forest
pima_glm_pred <- predict(pima_glm, newdata = pima_test)
pima_glm4_pred <- predict(pima_glm4, newdata = pima_test)

confusionMatrix(pima_glm_pred, criterion)
confusionMatrix(pima_glm4_pred, criterion)
```



```{r}
# fit random forest
pima_rf = rfe(x = diabetes ~ glucose + mass + age + pedigree,
                data = pima_train,
                trControl = trainControl(method = 'cv'),
                tuneGrid = data.frame(mtry = 3)
                )
```





9. Calculate feature importance using `varImp()`. See below.

```{r, echo = TRUE}
# determine feature importance
nearZeroVar(XX_pred)
```





In this section, we will again predict college graduation rates `Grad_Rate` from the `college_train` and `college_test` datasets. Here's how the first few rows of `college_train` should look:

```{r, results = 'asis', echo = FALSE, eval = TRUE}
knitr::kable(college_train[1:10,])
```

## Fitting

Using the basics steps in the Examples section above, fit each of three models, regression, decision trees, and random forests to the `college_train` data. your goal is to fit models predicting `Grad.Rate`

1. Look at the `college_train` dataset and decide which features you think are reasonable to include in a model.

2. Using `trainControl()`, set your training control method to `"none"`. Save your object as `ctrl_none`

3. Using `train()` fit a regression model called `grad_glm` predicting `Grad.Rate`, and using the features you think are reasonable.

4. Explore your `grad_glm` object by looking at `grad_glm$finalModel` and using `summary()`, what do you find?

5. Using `train()`, fit a decision tree model called `grad_dt` predicting `Grad.Rate` (using the same features as above).

- Set the complexity parameter to `cp = 0.01`

```{r}
grad_dt <- train(form = Grad.Rate ~ .,    # Make sure to specify your features here!
                 data = data_train,
                 method = "rpart",
                 trControl = train_control,
                 tuneGrid = expand.grid(cp = .01))   # Set complexity parameter
```


6. Explore your `grad_dt` object by looking at `grad_dt$finalModel` and plotting it with `plot(as.party(grad_dt$finalModel))`, what do you find?

7. Using `train()`, fit a random forest model called `grad_rf` predicting `Grad.Rate` (using the same features as above).

- Set the number of randomly sampled parameter to `mtry` = 2

```{r, eval = FALSE, echo = TRUE}
grad_rf <- train(form = Grad.Rate ~ .,    # Make sure to specify your features here!
                 data = college_train,
                 method = "rf",
                 trControl = train_control,
                 tuneGrid = expand.grid(mtry = 2))  # Set number of features randomly selected
```

8. Using `postResample()`, determine the fitting performance of each of your models.

9. Which one had the best fits? What was the fitting MAE of each model?

10. If you'd like to, try visualizing the fitting results using the plotting code shown in the Examples tab above.

##  Prediction

1. Using `postResample()`, determine the *prediction* performance of each of your models on the `college_test` data.

2. How does each model's prediction performance compare to its fitting performance? Is it worse? Better? The same?

3. Which of the three models has the best prediction performance?

4. If you had to use one of these three models in the real-world, which one would it be? Why?

5. If someone came to you and asked "If I use your model in the future to predict the graduation rate of a new college, how accurate do you think it would be?", what would you say?

# House Prices in King County, Washington

In this section, we will predict the prices of houses in King County Washington (home of Seattle, [which you can thank for this](https://qz.com/208457/a-cartographic-guide-to-starbucks-global-domination/)) using the `house_train` and `house_test` datasets. Here's how they should look:

```{r, results = 'asis', echo = FALSE, eval = TRUE}
knitr::kable(house_train[1:10,])
```

## Fitting

Your goal in the following models is to predict `price`

1. Look at the `house_train` dataset and decide which features you think are reasonable to include in a model.

2. Using `trainControl()`, set your training control method to `"none"`. Save your object as `ctrl_none`

3. Using `train()` fit a regression model called `price_glm` predicting `price` using the features you think are reasonable.

4. Explore your `price_glm` object by looking at `price_glm$finalModel` and using `summary()`, what do you find?

5. Using `train()`, fit a decision tree model called `price_dt` predicting `price` and using the same features as above.

- Set the complexity parameter to `cp = 0.01`

6. Explore your `price_dt` object by looking at `price_dt$finalModel` and plotting it with `plot(as.party(price_dt$finalModel))`, what do you find?

7. Using `train()`, fit a random forest model called `price_rf` predicting `price` and using the same features as above.

- Set the number of randomly sampled parameter to `mtry` = 2

8. Using `postResample()`, determine the fitting performance of each of your models

9. Which one had the best fits? What was the fitting MAE of each model?

10. If you'd like to, try visualizing the fitting results using the plotting code in the Examples

##  Prediction

1. Using `postResample()`, determine the prediction performance of each of your models on the `house_test` data.

2. How does each model's prediction performance compare to its fitting performance? Is it worse? Better? The same?

3. Which of the three models has the best prediction performance?

4. If you had to use one of these three models in the real-world, which one would it be? Why?

5. If someone came to you and asked "If I use your model in the future to predict house selling prices, how accurate do you think it would be?", what would you say?

# Exploring model tuning parameters

1. In all of your decision tree models so far, you have been setting the complexity parameter to 0.01. Try setting it to a largre value of 0.2 and see how your decision trees change (by plotting them). Do they get more or less complicated? How does increasing this value affect fitting and prediction performance? If you are interested in learning more about this parameter, look at the help menu with `?rpart.control`.

2. In each of your random forest models, you have been setting the `mtry` argument to 2. Try setting it to a larger value such as 5 and re-running your models. How does increasing this value affect fitting and prediction performance? If you are interested in learning more about this parameter, look at the help menu with `?randomForest`.

3. By default, the `train()` function uses 500 trees in `method = "rf"`. How do the number of trees affect performance? To answer this, try setting the number of trees to 1,000 (see example below) and re-evaluating your model's training and test performance. What do you find? What if you set the number of trees to just 10?

```{r, eval = FALSE}
# Create random forest model with 1000 trees

mod <- train(form = hwy ~ year + cyl + displ,
             data = data_train,
             method = "rf",
             trControl = train_control,
             ntree = 1000,   # use 1000 trees! (Instead of the default value of 500)
             tuneGrid = expand.grid(mtry = 2))

```

# Challenges

1. So far you've probably been using most, if not all, available features in predicting house sales. But imagine someone came to you and said "I need to know how much a set of new houses will sell for, but I only have access to three features `bedrooms`, `bathrooms`, and `sqft_living`. Which of your models should I use and how accurate will they be? How would you answer that question? Use your modelling techniques to find out!

2. Repeat your modelling process, but now do a classification task. Specifically, predict whether or not a house sells for at least \$1,000,000. To do this, you'll first need to create a new column called `million` in both your `house_train` and `house_test` datasets (the code below should help you). Then, use your best modelling techniques to make this prediction. How accurate are your models in predicting whether or not a house will sell for over $1,000,000? Don't forget to use the `confusionMatrix()` function instead of `postResample()` to evaluate your model's accuracy!

```{r}
# Add million column to house_train and house_test
#  A factor indicating whether or not a house sells for
#   over 1,000,0000

house_train <- house_train %>%
  mutate(million = factor(price > 1000000))

house_test <- house_test %>%
  mutate(million = factor(price > 1000000))
```
