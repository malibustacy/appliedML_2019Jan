<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Features</title>

<script src="Features_practical_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Features_practical_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Features_practical_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Features_practical_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Features_practical_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="Features_practical_files/navigation-1.1/tabsets.js"></script>
<link href="Features_practical_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Features_practical_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="practical.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Features</h1>
<h4 class="author"><em><table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'>
<col width='10%'>
<col width='10%'>
<tr style="border:none">
<td style="display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none" nowrap>
<font style='font-style:normal'>Applied Machine Learning with R</font> <br> <a href='https://therbootcamp.github.io/appliedML_2019Jan/'> <i class='fas fa-clock' style='font-size:.9em;' ></i> </a> <a href='https://therbootcamp.github.io'> <i class='fas fa-home' style='font-size:.9em;'></i> </a> <a href='mailto:therbootcamp@gmail.com'> <i class='fas fa-envelope' style='font-size: .9em;'></i> </a> <a href='https://www.linkedin.com/company/basel-r-bootcamp/'> <i class='fab fa-linkedin' style='font-size: .9em;'></i> </a> <a href='https://therbootcamp.github.io'> <font style='font-style:normal'>Basel R Bootcamp</font> </a>  
</td>
<td style="width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none">
<img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
</td>
</tr>
</table></em></h4>

</div>


<p align="center">
<img width="100%" src="image/wrongdata.gif" margin=0><br> <font style="font-size:10px">from <a href="https://dilbert.com/">dilbert.com</a></font>
</p>
<div id="section" class="section level2 tabset">
<h2></h2>
<div id="overview" class="section level3">
<h3>Overview</h3>
<p>By the end of this practical you will:</p>
<ol style="list-style-type: decimal">
<li>Understand the importance of the curse of dimensionality.</li>
<li>Know how to eliminate unwanted features.</li>
<li>Explore and use feature importance.</li>
<li>Use dimensionality reduction.</li>
</ol>
</div>
<div id="datasets" class="section level3">
<h3>Datasets</h3>
<table>
<thead>
<tr class="header">
<th align="left">File</th>
<th align="left">Rows</th>
<th align="left">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/pima_diabetes.csv">pima_diabetes</a></td>
<td align="left">724</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/violent_crime.csv">murders_crime</a></td>
<td align="left">1000</td>
<td align="left">102</td>
</tr>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/violent_crime.csv">violent_crime</a></td>
<td align="left">1000</td>
<td align="left">102</td>
</tr>
<tr class="even">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/nonviolent_crime.csv">nonviolent_crime</a></td>
<td align="left">1000</td>
<td align="left">102</td>
</tr>
</tbody>
</table>
<ul>
<li>The <code>pima_diabetes</code> is a subset of the <code>PimaIndiansDiabetes2</code> data set in the <code>mlbench</code> package. To see column descriptions, run the following:</li>
</ul>
<pre class="r"><code>library(mlbench)       # Load ISLR package
?PimaIndiansDiabetes2  # Look at help menu for College</code></pre>
<ul>
<li>The <code>murders_crime</code>, <code>violent_crime</code>, and <code>non_violent_crime</code> data are subsets of the Communities and Crime Unnormalized Data Set data set from the UCI Machine Learning Repository. To see column descriptions, visit this site: <a href="https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized">Communities and Crime Unnormalized Data Set</a></li>
</ul>
</div>
<div id="glossary" class="section level3">
<h3>Glossary</h3>
<table style="width:83%;">
<colgroup>
<col width="6%" />
<col width="11%" />
<col width="65%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Function</th>
<th align="left">Package</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>trainControl()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Define modelling control parameters</td>
</tr>
<tr class="even">
<td align="left"><code>train()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Train a model</td>
</tr>
<tr class="odd">
<td align="left"><code>predict(object, newdata)</code></td>
<td align="left"><code>stats</code></td>
<td align="left">Predict the criterion values of <code>newdata</code> based on <code>object</code></td>
</tr>
<tr class="even">
<td align="left"><code>postResample()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in regression tasks</td>
</tr>
<tr class="odd">
<td align="left"><code>confusionMatrix()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in classification tasks</td>
</tr>
<tr class="even">
<td align="left"><code>varImp()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Determine the model-specific importance of features</td>
</tr>
<tr class="odd">
<td align="left"><code>findCorrelation()</code>, <code>nearZeroVar()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Identify highly correlated and low variance features.</td>
</tr>
<tr class="even">
<td align="left"><code>rfe()</code>, <code>rfeControl()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Run and control recursive feature selection.</td>
</tr>
</tbody>
</table>
</div>
<div id="packages" class="section level3">
<h3>Packages</h3>
<table>
<thead>
<tr class="header">
<th align="left">Package</th>
<th align="left">Installation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>tidyverse</code></td>
<td align="left"><code>install.packages(&quot;tidyverse&quot;)</code></td>
</tr>
<tr class="even">
<td align="left"><code>tibble</code></td>
<td align="left"><code>install.packages(&quot;tibble&quot;)</code></td>
</tr>
<tr class="odd">
<td align="left"><code>caret</code></td>
<td align="left"><code>install.packages(&quot;caret&quot;)</code></td>
</tr>
</tbody>
</table>
</div>
<div id="cheatsheet" class="section level3">
<h3>Cheatsheet</h3>
<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf"> <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="Trulli" style="width:70%">
<figcaption>
hhttps://github.com/rstudio/cheatsheets/raw/master/caret.pdf
</figcaption>
</a>
</figure>
</div>
<div id="examples" class="section level3">
<h3>Examples</h3>
<pre class="r"><code># Step 0: Load packages-----------

library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(tibble)       # For advanced tibble functions
library(caret)        # For ML mastery 

# Step 1: Load, prepare, and explore data ----------------------

# read data
data &lt;- read_csv(&quot;1_Data/mpg_num.csv&quot;)

# Convert all characters to factors
data &lt;- data %&gt;%
  mutate_if(is.character, factor)

# Explore training data
data        # Print the dataset
dim(data)   # Print dimensions
names(data) # Print the names

# Step 2: Create training and test sets -------------

# Create train index
train_index &lt;- createDataPartition(criterion, 
                                   p = .8, 
                                   list = FALSE)

# Create training and test sets
data_train &lt;- data %&gt;% slice(train_index)
data_test &lt;- data %&gt;% slice(-train_index)

# split predictors and criterion
criterion_train &lt;- data_train %&gt;% select(hwy) %&gt;% pull()
predictors_train &lt;- data_train %&gt;% select(-hwy)
criterion_test &lt;- data_test %&gt;% select(hwy) %&gt;% pull()
predictors_test &lt;- data_test %&gt;% select(-hwy)

# Step 3: Clean data -------------

# Test for excessively correlated features
corr_matrix &lt;- cor(predictors_train)
corr_features &lt;- findCorrelation(corr_matrix)

# Remove excessively correlated features
predictors_train &lt;- predictors_train %&gt;% select(-corr_features)

# Test for near zero variance features
zerovar_features &lt;- nearZeroVar(predictors_train)

# Remove near zero variance features
predictors_train &lt;- predictors_train %&gt;% select(-zerovar_features)

# recombine data
data_train &lt;- predictors_train %&gt;% add_column(hwy = criterion_train)

# Step 4: Define training control parameters -------------

# Train using cross-validation
ctrl_cv &lt;- trainControl(method = &quot;cv&quot;) 

# Step 5: Fit models -------------

# Fit glm vanilla flavor
hwy_glm &lt;- train(form = hwy ~ .,
                 data = data_train,
                 method = &quot;glm&quot;,
                 trControl = ctrl_cv)

# Fit with pca transformation
hwy_glm_pca &lt;- train(form = hwy ~ .,
                     data = data_train,
                     method = &quot;glm&quot;,
                     trControl = ctrl_cv,
                     preProcess = c(&#39;pca&#39;))

# Fit scaling and centering
hwy_glm_sca &lt;- train(form = hwy ~ .,
                     data = data_train,
                     method = &quot;glm&quot;,
                     trControl = ctrl_cv,
                     preProcess = c(&#39;scale&#39;, &#39;center&#39;))

# Get fits
glm_fit     &lt;- predict(hwy_glm)
glm_pca_fit &lt;- predict(hwy_glm_pca)
glm_sca_fit &lt;- predict(hwy_glm_sca)

# Step 6: Evaluate variable importance -------------

# Run varImp()
imp_glm     &lt;- varImp(hwy_glm)
imp_glm_pca &lt;- varImp(hwy_glm_pca)
imp_glm_sca &lt;- varImp(hwy_glm_sca)

# Plot variable importance
plot(imp_glm)
plot(imp_glm_pca)
plot(imp_glm_sca)

# Step 7: Select variables -------------

# Select by hand in formula
hwy_glm_sel &lt;- train(form = hwy ~ cty,
                     data = data_train,
                     method = &quot;glm&quot;,
                     trControl = ctrl_cv)

# Select by hand in data
hwy_glm_sel &lt;- train(form = hwy ~ cty,
                     data = data_train %&gt;% 
                       select(-cyl, -displ, -year),
                     method = &quot;glm&quot;,
                     trControl = ctrl_cv)

# Select by reducing pca criterion ---

# Reduce criterion to 50% variance epxlained 
ctrl_cv_pca &lt;- trainControl(method = &quot;cv&quot;,
                            preProcOptions = list(thresh = 0.50)) 

# Refit model with update
hwy_glm_sel &lt;- train(form = hwy ~ cty,
                     data = data_train %&gt;% 
                       select(-cyl, -displ, -year),
                     method = &quot;glm&quot;,
                     trControl = ctrl_cv_pca,
                     preProcess = c(&#39;pca&#39;))

# Step 8: Recursive feature elimination -------------

# Feature elimination settings 
ctrl_rfe &lt;- rfeControl(functions = lmFuncs,  # linear model
                       method = &quot;cv&quot;,
                       verbose = FALSE)

# Run feature elimination
profile &lt;- rfe(x = predictors_train, 
               y = criterion_train,
               sizes = c(1, 2, 3),     # Features set sizes should be considered
               rfeControl = ctrl_rfe)

# plot result
trellis.par.set(caretTheme())
plot(profile, type = c(&quot;g&quot;, &quot;o&quot;))

# Step 9: Evaluate models -------------

# you know how...</code></pre>
</div>
</div>
<div id="tasks" class="section level1">
<h1>Tasks</h1>
<div id="a---setup" class="section level2">
<h2>A - Setup</h2>
<ol style="list-style-type: decimal">
<li><p>Open your <code>BaselRBootcamp</code> R project. It should already have the folders <code>1_Data</code> and <code>2_Code</code>. Make sure that the data file(s) listed in the <code>Datasets</code> section above are in your <code>1_Data</code> folder.</p></li>
<li><p>Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called <code>Features_practical.R</code> in the <code>2_Code</code> folder.</p></li>
<li><p>Using <code>library()</code> load the set of packages for this practical listed in the packages section above.</p></li>
</ol>
<pre class="r"><code>## NAME
## DATE
## Feature Practical

library(tidyverse)
library(caret)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>In the code below, we will load each of the data sets listed in the <code>Datasets</code> as new objects.</li>
</ol>
<pre class="r"><code># Pima Indians diabetes
pima_diabetes &lt;- read_csv(file = &quot;1_Data/pima_diabetes.csv&quot;)

# (Non-) violent crime statistics
violent_crime    &lt;- read_csv(file = &quot;1_Data/violent_crime.csv&quot;)
nonviolent_crime &lt;- read_csv(file = &quot;1_Data/nonviolent_crime.csv&quot;)

# murders crime statistics
murders_crime &lt;- read_csv(file = &quot;1_Data/murders_crime.csv&quot;)</code></pre>
</div>
<div id="b---pima-indians-diabetes" class="section level2">
<h2>B - Pima Indians Diabetes</h2>
<p>In this section, you will explore feature selection for the Pima Indians Diabetes data set. The Pima are a group of Native Americans living in Arizona. A genetic predisposition allowed this group to survive normally to a diet poor of carbohydrates for years. In the recent years, because of a sudden shift from traditional agricultural crops to processed foods, together with a decline in physical activity, made them develop the highest prevalence of type 2 diabetes and for this reason they have been subject of many studies.</p>
<ol style="list-style-type: decimal">
<li><p>Take a look at the first few rows of the pima diabetes data frame by printing then to the console.</p></li>
<li><p>Print the numbers of rows and columns of each data set using the <code>dim()</code> function.</p></li>
<li><p>Look at the names of the data frame with the <code>names()</code> function.</p></li>
<li><p>Open the data set in a new window using <code>View()</code>. Do they look OK?</p></li>
</ol>
<div id="splitting" class="section level4">
<h4>Splitting</h4>
<ol start="5" style="list-style-type: decimal">
<li>As always, before you do anything you need to make sure that you separate a hold-out data set for later. Create <code>pima_train</code> and <code>pima_test</code> using <code>createDataPartition()</code> with as little as <b>15% of cases going into the training set</b>. Also store the variable <code>diabetes</code> from the test set as a factor, which will be the criterion.</li>
</ol>
<pre class="r"><code># split index
train_index &lt;- createDataPartition(XX$XX, p = .15, list = FALSE)

# train and test sets
pima_train &lt;- XX %&gt;% slice(train_index)
pima_test  &lt;- XX %&gt;% slice(-train_index)

# test criterion
criterion &lt;- as.factor(pima_test$XX)</code></pre>
<pre class="r"><code># split index
train_index &lt;- createDataPartition(pima_diabetes$diabetes, p = .15, list = FALSE)

# train and test sets
pima_train &lt;- pima_diabetes %&gt;% slice(train_index)
pima_test  &lt;- pima_diabetes %&gt;% slice(-train_index)

# test criterion
criterion &lt;- as.factor(pima_test$diabetes)</code></pre>
</div>
<div id="remove-unwanted-features" class="section level4">
<h4>Remove unwanted features</h4>
<p>OK, with the training set, let’s get to work and remove some features.</p>
<ol start="6" style="list-style-type: decimal">
<li>First split the training data into a data frame holding the predictors and the criterion using the code below.</li>
</ol>
<pre class="r"><code># Select predictors
pima_train_pred &lt;- pima_train %&gt;% select(-XX)

# Select criterion
pima_train_crit &lt;- pima_train %&gt;% select(XX) %&gt;% pull()</code></pre>
<pre class="r"><code># Select predictors
pima_train_pred &lt;- pima_train %&gt;% select(-diabetes)

# Select criterion
pima_train_crit &lt;- pima_train %&gt;% select(diabetes) %&gt;% pull()</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Although, this data set is rather small and rather well curated, test if there are any excessively correlated features using <code>cor()</code> and <code>findCorrelation()</code> using the code below. Are there any?</li>
</ol>
<pre class="r"><code># determine correlation matrix
corr_matrix &lt;- cor(XX_pred)

# find excessively correlated variables
findCorrelation(corr_matrix)</code></pre>
<pre class="r"><code># determine correlation matrix
corr_matrix &lt;- cor(pima_train_pred)

# find excessively correlated variables
findCorrelation(corr_matrix)</code></pre>
<pre><code>integer(0)</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Now, test if there are any near-zero variance features Any of those?</li>
</ol>
<pre class="r"><code># find near zero variance predictors
nearZeroVar(XX_pred)</code></pre>
<pre class="r"><code># find near zero variance predictors
nearZeroVar(pima_train_pred)</code></pre>
<pre><code>integer(0)</code></pre>
</div>
<div id="feature-importance" class="section level4">
<h4>Feature importance</h4>
<p>After having retained all features in the previous section, this section explores feature selection on grounds of feature importance. To do this, we first need to fit our model. How about a simple logistic regression aka <code>method = 'glm'</code>?</p>
<ol start="9" style="list-style-type: decimal">
<li>Fit the <code>glm</code> to the training data.</li>
</ol>
<pre class="r"><code># fit regression
pima_glm &lt;- train(diabetes ~ .,
                data = XX,
                method = XX
                )</code></pre>
<pre class="r"><code># fit regression
pima_glm &lt;- train(diabetes ~ .,
                data = pima_train,
                method = &#39;glm&#39;)</code></pre>
<ol start="10" style="list-style-type: decimal">
<li>Evaluate feature importance using <code>varImp()</code>. The function will show importance on a scale from 0 (least important feature) to 100 (most important feature). You can set <code>scale = TRUE</code> to see absolute importance measures in t-values.</li>
</ol>
<pre class="r"><code># determine variable importance
varimp_glm &lt;- varImp(XX)

# print variable importance
varimp_glm

# print variable importance
plot(varimp_glm)</code></pre>
<pre class="r"><code># determine variable importance
varimp_glm &lt;- varImp(pima_glm)

# print variable importance
varimp_glm</code></pre>
<pre><code>glm variable importance

         Overall
glucose   100.00
mass       85.91
pregnant   70.41
pedigree   16.90
pressure    7.16
age         0.00</code></pre>
<pre class="r"><code># print variable importance
plot(varimp_glm)</code></pre>
<p><img src="Features_practical_files/figure-html/unnamed-chunk-19-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="model-comparison" class="section level4">
<h4>Model comparison</h4>
<ol start="11" style="list-style-type: decimal">
<li>Fit the glm a second time using only the four best features and store the result in a different fit object.</li>
</ol>
<pre class="r"><code># fit glm with best four features
pima_glm4 = train(diabetes ~ XX + YY + ZZ + AA,
                data = XX,
                method = XX)</code></pre>
<pre class="r"><code># fit glm with best four features
pima_glm4 = train(diabetes ~ glucose + mass + pregnant + pedigree,
                data = pima_train,
                method = &#39;glm&#39;)</code></pre>
<ol start="12" style="list-style-type: decimal">
<li>Using both fits, all features versus the best four, predict the criterion and evaluate the prediction using <code>confusionMatrix()</code>. Which model <code>glm</code> is better?</li>
</ol>
<pre class="r"><code># determine predictions for test data
pima_glm_pred &lt;- predict(XX, newdata = XX)
pima_glm4_pred &lt;- predict(XX, newdata = XX)

# evaluate the results
confusionMatrix(XX, reference = XX)
confusionMatrix(XX, reference = XX)</code></pre>
<pre class="r"><code># determine predictions for test data
pima_glm_pred &lt;- predict(pima_glm, newdata = pima_test)
pima_glm4_pred &lt;- predict(pima_glm4, newdata = pima_test)

# evaluate the results
confusionMatrix(pima_glm_pred, criterion)</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction neg pos
       neg 340  75
       pos  63 136
                                       
               Accuracy : 0.775        
                 95% CI : (0.74, 0.808)
    No Information Rate : 0.656        
    P-Value [Acc &gt; NIR] : 9.15e-11     
                                       
                  Kappa : 0.495        
 Mcnemar&#39;s Test P-Value : 0.349        
                                       
            Sensitivity : 0.844        
            Specificity : 0.645        
         Pos Pred Value : 0.819        
         Neg Pred Value : 0.683        
             Prevalence : 0.656        
         Detection Rate : 0.554        
   Detection Prevalence : 0.676        
      Balanced Accuracy : 0.744        
                                       
       &#39;Positive&#39; Class : neg          
                                       </code></pre>
<pre class="r"><code>confusionMatrix(pima_glm4_pred, criterion)</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction neg pos
       neg 339  75
       pos  64 136
                                        
               Accuracy : 0.774         
                 95% CI : (0.738, 0.806)
    No Information Rate : 0.656         
    P-Value [Acc &gt; NIR] : 1.66e-10      
                                        
                  Kappa : 0.492         
 Mcnemar&#39;s Test P-Value : 0.396         
                                        
            Sensitivity : 0.841         
            Specificity : 0.645         
         Pos Pred Value : 0.819         
         Neg Pred Value : 0.680         
             Prevalence : 0.656         
         Detection Rate : 0.552         
   Detection Prevalence : 0.674         
      Balanced Accuracy : 0.743         
                                        
       &#39;Positive&#39; Class : neg           
                                        </code></pre>
<ol start="13" style="list-style-type: decimal">
<li><p>You might have observed that the model with two features less is actually slightly better than the full model (if this is not the case, keep in mind that the partitioning of the dataset was done randomly, i.e., if you do it a second time, your results may slightly change). Why do you think is that the case?</p></li>
<li><p>Play around: Up the proportion dedicated to training or use a different model, e.g., <code>random forest</code>, and see whether things change.</p></li>
</ol>
</div>
</div>
<div id="c---murders" class="section level2">
<h2>C - Murders</h2>
<p>In this section, you will explore feature selection but use a data set with very different properties. The data combines socio-economic data from the ’90 Census, data from Law Enforcement Management and Admin Stats survey, and crime data from the FBI, allowing to analyse the relationship between various socio-demographic variables and whether murders have been committed (<code>murders</code>), the criterion of this exercise.</p>
<ol style="list-style-type: decimal">
<li><p>Take a look at the first few rows of the <code>murders_crime</code> data frame by printing then to the console.</p></li>
<li><p>Print the numbers of rows and columns of each data set using the <code>dim()</code> function.</p></li>
<li><p>Look at the names of the data frame with the <code>names()</code> function.</p></li>
<li><p>Open the data set in a new window using <code>View()</code>. Do they look OK?</p></li>
</ol>
<div id="splitting-1" class="section level4">
<h4>Splitting</h4>
<ol start="5" style="list-style-type: decimal">
<li>Again, before you do anything you need to make sure that you separate a hold-out data set for later. Create <code>murders_train</code> and <code>murders_test</code> using <code>createDataPartition()</code> with again as little as <b>25% of cases going into the training set</b>. Also store the variable <code>murders</code> from the test set as a factor, which will be the criterion.</li>
</ol>
<pre class="r"><code># split index
train_index &lt;- createDataPartition(murders_crime$murders, p = .25, list = FALSE)

# train and test sets
murders_train &lt;- murders_crime %&gt;% slice(train_index)
murders_test  &lt;- murders_crime %&gt;% slice(-train_index)

# test criterion
criterion &lt;- as.factor(murders_test$murders)</code></pre>
</div>
<div id="remove-unwanted-features-1" class="section level4">
<h4>Remove unwanted features</h4>
<p>OK, with the training set, let’s get to work and remove some features.</p>
<ol start="6" style="list-style-type: decimal">
<li>First split the training data into a data frame holding the predictors and the criterion using the code below.</li>
</ol>
<pre class="r"><code># Select predictors
murders_train_pred &lt;- murders_train %&gt;% select(-murders)

# Select criterion
murders_train_crit &lt;- murders_train %&gt;% select(murders) %&gt;% pull()</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Test if there are any excessively correlated features using <code>cor()</code> and <code>findCorrelation()</code> using the code below. Are there any this time?</li>
</ol>
<pre class="r"><code># determine correlation matrix
corr_matrix &lt;- cor(murders_train_pred)

# find excessively correlated variables
findCorrelation(corr_matrix)</code></pre>
<pre><code> [1] 11 17 27 30 40 41 44 48 49 53 54 55 57 58 59 60 61 64 71 81 84 85 87
[24]  7  8 13 20 21 31 43  1 62 67 79 51 91 56</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Remove the excessively correlated features from the training predictor set.</li>
</ol>
<pre class="r"><code># remove features
murders_train_pred &lt;- murders_train_pred %&gt;% select(- XX)</code></pre>
<pre class="r"><code># remove features
murders_train_pred &lt;- murders_train_pred %&gt;% 
  select(-findCorrelation(corr_matrix))</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>Test if there are any near-zero variance features Any of those this time?</li>
</ol>
<pre class="r"><code># find near zero variance predictors
nearZeroVar(murders_train_pred)</code></pre>
<pre><code>integer(0)</code></pre>
<ol start="10" style="list-style-type: decimal">
<li>There were plenty of excessively correlated features but no near-zero variance predictors. Bind the reduced predictor set together with the criterion into a new, clean version of the training set.</li>
</ol>
<pre class="r"><code># clean training set
murders_train_clean &lt;- murders_train_pred %&gt;% 
  add_column(murders = XX)</code></pre>
<pre class="r"><code># combine clean predictor set with criterion
murders_train_clean &lt;- murders_train_pred %&gt;% 
  add_column(murders = murders_train_crit)</code></pre>
</div>
<div id="model-comparison-1" class="section level4">
<h4>Model comparison</h4>
<ol start="11" style="list-style-type: decimal">
<li>Fit a <code>glm</code> twice, once using the original training set and once using the clean training set, and store the fits in separate objects.</li>
</ol>
<pre class="r"><code># fit glm with best four features
murders_glm = train(murders ~ .,
                    data = XX,
                    method = &#39;glm&#39;)

# fit glm with best four features
murders_glm_clean = train(murders ~ .,
                          data = XX,
                          method = &#39;glm&#39;)</code></pre>
<pre class="r"><code># fit glm with best four features
murders_glm = train(murders ~ .,
                    data = murders_train,
                    method = &#39;glm&#39;)

# fit glm with best four features
murders_glm_clean = train(murders ~ .,
                          data = murders_train_clean,
                          method = &#39;glm&#39;)</code></pre>
<ol start="12" style="list-style-type: decimal">
<li>You probably have noticed warning messages. They concern the very fact the features in both data sets, but especially the non-clean set, are still too highly correlated. Go ahead and evaluate the fits on the hold-out set. Which set of features predicts better?</li>
</ol>
<pre class="r"><code># determine predictions for test data
murders_pred &lt;- predict(murders_glm, newdata = murders_test)
murders_clean_pred &lt;- predict(murders_glm_clean, newdata = murders_test)

# evaluate the results
confusionMatrix(murders_pred, criterion)</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  445 196
       yes 185 541
                                        
               Accuracy : 0.721         
                 95% CI : (0.697, 0.745)
    No Information Rate : 0.539         
    P-Value [Acc &gt; NIR] : &lt;2e-16        
                                        
                  Kappa : 0.44          
 Mcnemar&#39;s Test P-Value : 0.608         
                                        
            Sensitivity : 0.706         
            Specificity : 0.734         
         Pos Pred Value : 0.694         
         Neg Pred Value : 0.745         
             Prevalence : 0.461         
         Detection Rate : 0.326         
   Detection Prevalence : 0.469         
      Balanced Accuracy : 0.720         
                                        
       &#39;Positive&#39; Class : no            
                                        </code></pre>
<pre class="r"><code>confusionMatrix(murders_clean_pred, criterion)</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  464 204
       yes 166 533
                                        
               Accuracy : 0.729         
                 95% CI : (0.705, 0.753)
    No Information Rate : 0.539         
    P-Value [Acc &gt; NIR] : &lt;2e-16        
                                        
                  Kappa : 0.458         
 Mcnemar&#39;s Test P-Value : 0.0544        
                                        
            Sensitivity : 0.737         
            Specificity : 0.723         
         Pos Pred Value : 0.695         
         Neg Pred Value : 0.763         
             Prevalence : 0.461         
         Detection Rate : 0.339         
   Detection Prevalence : 0.489         
      Balanced Accuracy : 0.730         
                                        
       &#39;Positive&#39; Class : no            
                                        </code></pre>
</div>
<div id="data-compression-with-pca" class="section level4">
<h4>Data compression with PCA</h4>
<ol start="13" style="list-style-type: decimal">
<li>Given the high features correlations it is sensible to compress the data using <code>principal component analysis</code> (PCA). Create a third fit object using the original training set with <code>preProcess = c('pca')</code> and <code>trControl = trainControl(preProcOptions = list(thresh = 0.8))</code> in the training function. The first argument tells R to use PCA, the second specifies that exactly 80% of the variance should be retained.</li>
</ol>
<pre class="r"><code># fit glm with best four features
murders_glm_pca = train(murders ~ .,
                        data = murders_train,
                        method = &#39;glm&#39;,
                        preProcess = c(&#39;pca&#39;),
                        trControl = trainControl(preProcOptions = list(thresh = 0.8)))</code></pre>
<ol start="14" style="list-style-type: decimal">
<li>Compare the prediction performance to the previous to the previous two models.</li>
</ol>
<pre class="r"><code># determine predictions for test data
murders_pca &lt;- predict(murders_glm_pca, newdata = murders_test)

# evaluate the results
confusionMatrix(murders_pca, criterion)</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  471 171
       yes 159 566
                                        
               Accuracy : 0.759         
                 95% CI : (0.735, 0.781)
    No Information Rate : 0.539         
    P-Value [Acc &gt; NIR] : &lt;2e-16        
                                        
                  Kappa : 0.515         
 Mcnemar&#39;s Test P-Value : 0.545         
                                        
            Sensitivity : 0.748         
            Specificity : 0.768         
         Pos Pred Value : 0.734         
         Neg Pred Value : 0.781         
             Prevalence : 0.461         
         Detection Rate : 0.345         
   Detection Prevalence : 0.470         
      Balanced Accuracy : 0.758         
                                        
       &#39;Positive&#39; Class : no            
                                        </code></pre>
<ol start="15" style="list-style-type: decimal">
<li>Play around: Alter the amount of variance explained by the <code>PCA</code> (using <code>thresh</code>), increase the proportion dedicated to training, use a different model, e.g., <code>random forest</code>, and see whether things change.</li>
</ol>
</div>
<div id="feature-importance-1" class="section level4">
<h4>Feature importance</h4>
<ol start="16" style="list-style-type: decimal">
<li>Evaluate the feature importance for each of the three models.</li>
</ol>
<pre class="r"><code># determine variable importance
varimp_glm &lt;- varImp(murders_glm)
varimp_glm_clean &lt;- varImp(murders_glm_clean)
varimp_glm_pca &lt;- varImp(murders_glm_pca)

# print variable importance
varimp_glm</code></pre>
<pre><code>glm variable importance

  only 20 most important variables shown (out of 99)

                   Overall
population           100.0
PctBornSameState      93.5
OwnOccMedVal          92.9
PctImmigRec10         92.0
OwnOccHiQuart         86.8
PctImmigRec8          86.0
OwnOccLowQuart        85.1
PctWorkMom            83.6
pctUrban              82.5
numbUrban             82.0
PctRecImmig10         73.8
RentMedian            71.9
pctWFarmSelf          68.1
PersPerRentOccHous    66.5
PctSameState85        64.1
NumStreet             62.9
MedRent               61.1
PctRecImmig8          59.7
PctOccupManu          58.7
RentLowQ              57.6</code></pre>
<pre class="r"><code>varimp_glm_clean</code></pre>
<pre><code>glm variable importance

  only 20 most important variables shown (out of 64)

                 Overall
LandArea           100.0
NumStreet           67.1
PctHousOccup        64.0
racePctWhite        58.1
indianPerCap        55.6
PctOccupManu        53.6
PctHousNoPhone      53.0
PctUnemployed       52.9
householdsize       47.8
pctWWage            47.6
PctVacMore6Mos      45.8
PctBornSameState    44.9
pctUrban            44.6
racePctAsian        41.5
PopDens             40.1
blackPerCap         34.5
PctImmigRecent      33.7
racepctblack        31.6
PctOccupMgmtProf    31.5
AsianPerCap         30.8</code></pre>
<pre class="r"><code>varimp_glm_pca</code></pre>
<pre><code>glm variable importance

     Overall
PC1   100.00
PC2    76.90
PC3    61.39
PC6    54.94
PC5    51.81
PC4    51.49
PC8    44.86
PC9    15.57
PC10   14.30
PC7     5.66
PC11    0.00</code></pre>
<pre class="r"><code># print variable importance
plot(varimp_glm)</code></pre>
<p><img src="Features_practical_files/figure-html/unnamed-chunk-37-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(varimp_glm_clean)</code></pre>
<p><img src="Features_practical_files/figure-html/unnamed-chunk-37-2.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(varimp_glm_pca)</code></pre>
<p><img src="Features_practical_files/figure-html/unnamed-chunk-37-3.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="17" style="list-style-type: decimal">
<li>Can you select a set of features based on feature performance that reliably beats predictions based on the pca-compressed training set?</li>
</ol>
</div>
</div>
<div id="z---violent-non-violent-crime-data" class="section level2">
<h2>Z - Violent &amp; Non-violent Crime Data</h2>
<ol style="list-style-type: decimal">
<li><p>Run similar analyses for the Violent and non-violent crime data sets predicting either the number of violent crimes per 100k inhabitants (<code>ViolentCrimesPerPop</code>) or the number of non-violent crimes per 100k inhabitants (<code>nonViolPerPop</code>). This time the criterion is numeric, implying regression rather than classification. The features are identical to the murders analysis of the previous section.</p></li>
<li><p>Use the computer to find a good set of features using recursive feature elimination with <code>rfe()</code>.</p></li>
</ol>
<pre class="r"><code># split index
train_index &lt;- createDataPartition(violent_crime$ViolentCrimesPerPop, 
                                   p = .8, 
                                   list = FALSE)

# train and test sets
violent_train &lt;- violent_crime %&gt;% slice(train_index)
violent_test  &lt;- violent_crime %&gt;% slice(-train_index)

# remove extreme correlations (OPTIONAL)
# predictors &lt;- violent_train %&gt;% select(-ViolentCrimesPerPop)
# predictors &lt;- predictors %&gt;% select(-findCorrelation(cor(predictors)))
# violent_train_clean &lt;- predictors %&gt;% 
#   add_column(ViolentCrimesPerPop = violent_train$ViolentCrimesPerPop)

# Feature elimination settings 
ctrl_rfe &lt;- rfeControl(functions = lmFuncs,  # linear model
                          method = &quot;cv&quot;,
                          verbose = FALSE,
                          rerank = FALSE)

# Run feature elimination
profile &lt;- rfe(x = violent_train %&gt;% select(-ViolentCrimesPerPop), 
               y = violent_train$ViolentCrimesPerPop,
               sizes = 1:(ncol(violent_train_clean)-1), # Features set sizes
               rfeControl = ctrl_rfe)

# inspect cross-validation as a function of performance
plot(profile)</code></pre>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
