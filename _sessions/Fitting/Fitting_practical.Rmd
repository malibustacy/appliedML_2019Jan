---
title: "Fitting"
subtitle: "Model fitting with regression"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
output:
  html_document:
    css: practical.css
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)
```

```{r, echo = FALSE, fig.align = 'center', eval = TRUE, out.width = "70%", fig.cap="Source: wikipedia"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png")
```

## {.tabset}

### Overview

In this practical you'll practice the basics of fitting and exploring regression models in R.

By the end of this practical you will know how to:

1. Fit regression to training data.
2. Explore your object with generic functions.
3. Evaluate its fitting performance using accuracy measures such as MSE and MAE.
4. Explore the effects of adding additional features.

### Datasets

```{r, eval = TRUE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
```

|File  |Rows | Columns |
|:----|:-----|:------|
|[college_train.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/_sessions/_data/college_train.csv)| 1000 | 21|

- The `college_train` data are taken from the `College` dataset in the `ISLR` package. To see column descriptions, run the following:

```{r, eval = FALSE}
library(ISLR)   # Load ISLR package
?College        # Look at help menu for College
```

### Glossary

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `trainControl()`|`caret`|    Define modelling control parameters| 
| `train()`|`caret`|    Train a model|
| `predict(object, newdata)`|`base`|    Predict the criterion values of `newdata` based on `object`|
| `postResample()`|`caret`|   Calculate aggregate model performance in regression tasks|
| `confusionMatrix()`|`caret`|   Calculate aggregate model performance in classification tasks| 

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`caret`|`install.packages("caret")`|

### Cheatsheet

<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf">
  <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet-200x155@2x.png" alt="Trulli" style="width:70%">
  <figcaption>hhttps://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption></a>
</figure>

### Examples

```{r, eval = FALSE, echo = TRUE}
# Fitting and evaluating a regression model ------------------------------------

# Step 0: Load packages-----------

library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 

# Step 1: Load and Clean, and Explore Training data ----------------------

# I'll use the mpg dataset from the dplyr package in this example
#  no need to load an external dataset

data_train <- read_csv("1_Data/mpg_train.csv")

# Convert all characters to factor
#  Some ML models require factors

data_train <- data_train %>%
  mutate_if(is.character, factor)

# Explore training data

data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Step 2: Define training control parameters -------------

# In this case, I will set method = "none" to fit to 
#  the entire dataset without any fancy methods

train_control <- trainControl(method = "none") 

# Step 3: Train model: -----------------------------
#   Criterion: hwy
#   Features: year, cyl, displ, trans

# Regression

hwy_glm <- train(form = hwy ~ year + cyl + displ + trans,
                 data = data_train,
                 method = "glm",
                 trControl = train_control)

# Look at summary information
summary(hwy_glm)

# Step 4: Access fit ------------------------------

# Save fitted values
glm_fit <- predict(hwy_glm)

# Define data_train$hwy as the true criterion
criterion <- data_train$hwy

# Regression Fitting Accuracy
postResample(pred = glm_fit, 
             obs = criterion)

#     RMSE Rsquared      MAE 
# 3.246182 0.678465 2.501346 

# On average, the model fits are 2.8 away from the true
#  criterion values

# Step 5: Visualise Accuracy -------------------------

# Tidy competition results
accuracy <- tibble(criterion = criterion,
                   Regression = glm_fit) %>%
               gather(model, prediction, -criterion) %>%
  
  # Add error measures
  mutate(se = prediction - criterion,
         ae = abs(prediction - criterion))


# Calculate summaries
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of criterion versus predictions

ggplot(data = accuracy,
       aes(x = criterion, y = prediction, col = model)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Predicting mpg$hwy",
       subtitle = "Black line indicates perfect performance")

# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Distributions of Fitting Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))
```

# Tasks

## A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data file(s) listed in the `Datasets` section above are in your `1_Data` folder

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `Fitting_practical.R` in the `2_Code` folder.  
3. Using `library()` load the set of packages for this practical listed in the packages section above.

```{r, echo = TRUE, message = FALSE}
## NAME
## DATE
## Fitting Practical

library(tidyverse)
library(caret)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(tidyverse)
```

4. For this practical, we'll use a dataset of 388 U.S. Colleges.The data is stored in `college_train.csv`. Using the following template, load the dataset into R as `college_train`:

```{r, echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE}
# Load in college_train.csv data as college_train

college_train <- read_csv(file = "1_Data/college_train.csv")
```

```{r, eval = TRUE, echo = FALSE}
college_train <- read_csv(file = "../../1_Data/college_train.csv")
```

5. Take a look at the first few rows of the dataset by printing it to the console.

```{r}
college_train
```

6. Print the numbers of rows and columns using the `dim()` function


```{r, echo = TRUE, eval = FALSE}
# Print number of rows and columns of college_train

dim(XXX)
```


```{r}
# Print number of rows and columns of college_train

dim(college_train)
```

7. Look at the names of the dataframe with the `names()` function

```{r, echo = TRUE, eval = FALSE}
names(XXX)
```

```{r}
names(college_train)
```

8. Open the dataset in a new window using `View()`. How does it look?

```{r, echo = TRUE, eval = FALSE}
View(XXX)
```

```{r}
View(college_train)
```

9. We need to do a little bit of data cleaning before starting. Specifically, we need to convert all character columns to factors: Do this by running the following code:

```{r, echo = TRUE}
# Convert character to factor

college_train <- college_train %>%
          mutate_if(is.character, factor)
```

## B - Determine sampling procedure

In `caret`, we always need to define computational nuances of training using the `trainControl()` function. Because we're learning the basics of fitting, we'll set `method = "none"` (Note that you would almost never do this for a real prediction task, you'll see why later!)

```{r}
# Set training resampling method to "none" to keep everything super simple
#   for demonstration purposes

ctrl_none <- trainControl(method = "none") 
```

# Regression

## C - Fit a regression model

1. Using the code below, fit a regression model predicting graduation rate (`Grad.Rate`) as a function of one feature `PhD`, the percent of faculty with PhDs). Save the result as an object `Grad.Rate_glm`

- Set the `form` argument to `Grad.Rate ~ PhD`
- Set the `data` argument to your training data `college_train`
- Set the `method` argument to `"glm"` for regression
- Set the `trControl` argument to `ctrl_none`, the object you created previously

```{r, echo = TRUE, eval = FALSE}
# Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD

Grad.Rate_glm <- train(form = XX ~ XX,
                       data = XX,
                       method = "XX",
                       trControl = XX)
```


```{r}
# Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD

Grad.Rate_glm <- train(form = Grad.Rate ~ PhD,
                       data = college_train,
                       method = "glm",
                       trControl = ctrl_none)
```

2. Explore the model using the `summary()` function.
- Set the main argument to `Grad.Rate_glm`

```{r, echo = TRUE, eval = FALSE}
# Show summary information from the regression model

summary(XXX)
```


```{r, echo = TRUE}
# Show summary information from the regression model

summary(Grad.Rate_glm)
```

3. Look at the results, how do you interpret the regression coefficients? What is the general relationship between PhD and graduation rates? Does this make sense?

```{r}
# For every increase of one in PhD (the percent of faculty with a PhD), the expected graduation rate increases by 0.33
```

4. Now it's time to save the model fits! Do this by running the following code to save the fitted values as `glm_fit`
- Set the main argument to `Grad.Rate_glm`

```{r, echo = TRUE, eval = FALSE}
# Get fitted values from the Grad.Rate_glm model and save as glm_fit

glm_fit <- predict(XXX)
```

```{r}
# Get fitted values from the model and save as glm_fit

glm_fit <- predict(Grad.Rate_glm)
```

5. Print your `glm_fit` object - what are these values? Do they look reasonable? That is, are they in the range of what you expect the criterion to be?

```{r, echo = TRUE}
# Print glm_fit

glm_fit
```

```{r}
# Yes, values appear to be within 40 and 80, which is what we expect from the truth population.
```

## D - Evaluate accuracy

1. Now it's time to compare your model fits to the true values, we'll start by defining the vector `criterion` as the graduation rates.

```{r, echo = TRUE, eval = TRUE}
# Define criterion as Grad.Rate

criterion <- college_train$Grad.Rate
```

2. Let's quantify our model's fitting results. To do this, we'll use the `postResample()` function, with the fitted values as the prediction, and the criterion as the observed values:

- Set the `pred` argument to `glm_fit` (your fitted values)
- Set the `obs` argument to `criterion` (a vector of the criterion values)

```{r, echo = TRUE, eval = FALSE}
# Regression Fitting Accuracy

postResample(pred = XXX,   # Fitted values 
             obs = XXX)  # criterion values
```

```{r}
# Regression Fitting Accuracy

postResample(pred = glm_fit,   # Fitted values 
             obs = criterion)  # criterion values
```

3. You'll see three values here, the easiest to understand is MAE which stands for "Mean Absolute Error" -- in other words, "on average how far are the predictions from the true values?" A value of 0 means perfect prediction, so small values are good! How do you interpret these results?

```{r}
# On average, the model fits are 12.8633 away from the true values.
#  Whether this is 'good' or not depends on you :)
```

4. Now we're ready to do some plotting. But first, we need to re-organise the data a bit. We'll create two dataframes
  - `accuracy`: Raw absolute errors
  - `accuracy_agg` Aggregate (i.e.; mean) absolute errors

```{r, echo = TRUE}
# accuracy: a dataframe of raw absolute errors
accuracy <- tibble(criterion = criterion,
                   Regression = glm_fit) %>%
                gather(model, prediction, -criterion) %>%
  
  # Add error measures
  mutate(ae = abs(prediction - criterion))

# accuracy_agg: Dataframe of aggregate errors
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)
```

5. Print the `accuracy` and `accuracy_agg` objects to see how they look.

```{r, echo = TRUE, eval = FALSE}
accuracy
accuracy_agg
```


```{r}
accuracy %>% head() # Just printing the first few rows

accuracy_agg
```

6. Using the code below, create a scatterplot showing the relationship between the true criterion values and the model fits

```{r, out.width = "60%", echo = TRUE, eval = FALSE}
# Plot A) Scatterplot of criterion versus predictions

ggplot(data = accuracy,
       aes(x = criterion, y = prediction)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Regression: One Feature",
       subtitle = "Line indicates perfect performance",
       x = "True Graduation Rates",
       y = "Fitted Graduation Rates") +
  xlim(0, 120) + 
  ylim(0, 120)
```

7. Look at the plot, how do you interpret this? Do you think the model did well or not in fitting graduation rates?

```{r}
# No the model is not great, values do not fall very closely to the black diagonal line.
```

8. Let's create a new violin plot showing the distribution of absolute errors of the model:

```{r, echo = TRUE, eval = FALSE}
# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Distributions of Fitting Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))
```

9. What does the plot show you about the model fits? On average, how far away were the model fits from the true values?

```{r}
# On average, the model fits are 12.86 away from the true criterion values.
#  However, there is also quite a bit of variability
```


## F - Add more features

So far we have only used one feature (`PhD`), to predict `Grad.Rate`. Let's try again, but now we'll use a total of four features:

- `PhD`: The percent of faculty with a PhD
- `Room.Board`: Room and board costs
- `Terminal`: Percent of faculty with a terminal degree
- `S.F.Ratio`: Student to faculty ratio

1. Using the same steps as above, create a regression model `Grad.Rate_glm` which predicts `Grad.Rate` using all 4 features (you can also call it something else if you want to save your original model!).

- Set the `form` argument to `Grad.Rate ~ PhD + Room.Board + Terminal + S.F.Ratio`
- Set the `data` argument to your training data `college_train`
- Set the `method` argument to `"glm"` for regression
- Set the `trControl` argument to `ctrl_none`

```{r, echo = TRUE, eval = FALSE}
# Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD, Room.Board, Terminal, S.F.Ratio

Grad.Rate_glm <- train(form = XXX ~ XXX + XXX + XXX + XXX,
                       data = XXX,
                       method = "XXX",
                       trControl = XXX)
```

```{r}
# Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD, Room.Board, Terminal, S.F.Ratio

Grad.Rate_glm <- train(form = Grad.Rate ~ PhD + Room.Board + Terminal + S.F.Ratio,
                       data = college_train,
                       method = "glm",
                       trControl = ctrl_none)
```

2. Explore your model using `summary()`, which features seem to be important?
- Set the main argument to `Grad.Rate_glm`

```{r, echo = TRUE, eval = FALSE}
summary(XXX)
```

```{r}
summary(Grad.Rate_glm)
```


3. Save the model fits as a new object `glm_fit`
- Set the main argument of `predict()` to your `Grad.Rate_glm` model


```{r, echo = TRUE, eval = FALSE}
# Save new model fits
glm_fit <- predict(XXX)
```

```{r}
# Save new model fits
glm_fit <- predict(Grad.Rate_glm)
```



3. By comparing the model fits to the true criterion values using `postResample()` calculate the Mean Absolute Error (MAE) of your new model that uses 4 features. How does this compare to your previous model that only used 1 feature?
- Set the `pred` argument to `glm_fit`, your model fits
- Set the `obs` argument to `criterion`, a vector of the true criterion values

```{r, echo = TRUE, eval = FALSE}
# New model fitting accuracy

postResample(pred = XXX,   # Fitted values 
             obs = XXX)  # criterion values
```

```{r}
# Save new model fits
glm_fit <- predict(Grad.Rate_glm)

# New model fitting accuracy

postResample(pred = glm_fit,   # Fitted values 
             obs = criterion)  # criterion values
```

```{r}
# The new MAE value is 11.779, it's better (smaller) than the previous model, but still not great (in my opinion)
```

4. (Optional). Create a scatter plot showing the relationship between your new model fits and the true values. How does this plot compare to your previous one?

```{r, out.width = "60%"}
# accuracy: a dataframe of raw absolute errors
accuracy <- tibble(criterion = criterion,
                   Regression = glm_fit) %>%
                gather(model, prediction, -criterion) %>%
  
  # Add error measures
  mutate(ae = abs(prediction - criterion))

# accuracy_agg: Dataframe of aggregate errors
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of criterion versus predictions

ggplot(data = accuracy,
       aes(x = criterion, y = prediction)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Regression: Four Features",
       subtitle = "Line indicates perfect performance",
       x = "True Graduation Rates",
       y = "Fitted Graduation Rates") +
  xlim(0, 120) + 
  ylim(0, 120)
```

5. (Optional). Create a violin plot showing the distribution of absolute errors. How does this compare to your previous one?

```{r}
# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Distributions of Fitting Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))
```

## F - Use all features

Alright now it's time to use all features available!

1. Using the same steps as above, create a regression model `glm_fit` which predicts `Grad.Rate` using *all* features in the dataset.

- Set the `form` argument to `Grad.Rate ~ .`
- Set the `data` argument to the training data `college_train`
- Set the `method` argument to `"glm"` for regression
- Set the `trControl` argument to `ctrl_none`

```{r, echo = TRUE, eval = FALSE}
Grad.Rate_glm <- train(form = XXX ~ .,
                       data = XXX,
                       method = "glm",
                       trControl = XXX)
```

```{r}
Grad.Rate_glm <- train(form = Grad.Rate ~ .,
                       data = college_train,
                       method = "glm",
                       trControl = ctrl_none)
```

2. Explore your model using `summary()`, which features seem to be important?

```{r, echo = TRUE, eval = FALSE}
summary(XXX)
```

```{r}
summary(Grad.Rate_glm)
```

3. Save the model fits as a new object `glm_fit`
- Set the main argument of `predict()` to your `Grad.Rate_glm` model

```{r, echo = TRUE, eval = FALSE}
# Save new model fits
glm_fit <- predict(XXX)
```

```{r}
# Save new model fits
glm_fit <- predict(Grad.Rate_glm)
```

3. What is the Mean Absolute Error (MAE) of your new model that uses 17 features? How does this compare to your previous model that only used 1 feature?

4. (Optional). Create a scatter plot showing the relationship between your new model fits and the true values. How does this plot compare to your previous one?

5. (Optional). Create a violin plot showing the distribution of absolute errors. How does this compare to your previous one?

# Classification

## G - Make sure your criterion is a factor!

Now it's time to do a classification task! Recall that in a classification task, we are predicting a category, not a continuous number. In this task, we'll predict whether or not a college is Private or Public, this is stored as the variable `college_train$Private`

1. In order to do classification training with `caret`, all you need to do is make sure that the criterion is coded as a factor. To test whether is coded as a factor, you can look at its class as follows:

```{r, echo = TRUE}
# Look at the class of the variable Private, should be a factor!

class(college_train$Private)
```

2. Now, we'll save the Private column as a new object called `criterion`

```{r, echo = TRUE}
# Define criterion as college_train$Private

criterion <- college_train$Private
```

## H - Fit a classification model

1. Using `train()`, create `Private_glm`, a regression model predicting the variable `Private`
- Set the `form` argument to `Private ~ .`
- Set the `data` argument to the training data `college_train`
- Set the `method` argument to `"glm"`
- Set the `trControl` argument to `ctrl_none`

```{r, echo = TRUE, eval = FALSE}
# Fit regression model predicting Private

Private_glm <- train(form = XXX ~ .,
                     data = XXX,
                     method = "XXX",
                     trControl = XXX)
```


```{r}
# Fit regression model predicting private
Private_glm <- train(form = Private ~ .,
                     data = college_train,
                     method = "glm",
                     trControl = ctrl_none)
```

2. Explore the `Private_glm` object using the `summary()` function.
- Set the main argument to `Private_glm`


```{r, echo = TRUE, eval = FALSE}
# Explore the Private_glm object
summary(XXX)
```

```{r}
# Show summary information from the regression model

summary(Private_glm)
```

3. Look at the results, how do you interpret the regression coefficients? Which features seem important in predicting whether a school is private or not?

```{r}
# Looking at the Z statistics, Outstate, Enroll and S.F.Ratio (...) have quite large z-statistics
```

## I - Access classification model accuracy

1. Now it's time to save the model fits! Do this by running the following code to save the fitted values as `glm_fit`
- Set the `object` argument to your `Private_glm` object

```{r, echo = TRUE, eval = FALSE}
# Get fitted values from the Private_glm object

glm_fit <- predict(XXX)
```

2. Print your `glm_fit` object - what are these values? Do they look reasonable?

```{r}
# Print glm_fit

glm_fit
```

3. Now it's time to calculate model accuracy, to do this, we will use a new function called `confusionMatrix()`. This function compares model predictions to a 'reference' (in our case, the criterion, and returns several summary statistics). In the code below, we'll use `glm_fit` as the model predictions, and our already defined `criterion` vector as the reference (aka, truth)
- Set the `data` argument to your `glm_fit` values
- Set the `reference` argument to the `criterion` values

```{r, eval = FALSE, echo = TRUE}
# Show accuracy of glm_fit versus the true criterion values

confusionMatrix(data = XXX,      # This is the prediction!
                reference = XXX) # This is the truth!
```


```{r}
# Show accuracy of glm_fit versus the true values

confusionMatrix(data = glm_fit,        # This is the prediction!
                reference = criterion) # This is the truth!
```

4. Look at the results, what is the overall accuracy of the model? How do you interpret this?

5. What is the sensitivity? How do you interpret this number?

6. What is the specificity? How do you interpret this number?

7. To visualize the accuracy of your classification models, use the following code to create a bar plot:

```{r}
# Get overall accuracy from regression model
glm_accuracy <- confusionMatrix(data =  glm_fit,  
                                reference = criterion)$overall[1]

# Combine results into one table
accuracy <- tibble(Regression = glm_accuracy) %>%
              gather(model, accuracy)


# Plot the results!
ggplot(accuracy, aes(x = model, y = accuracy, fill = model)) + 
  geom_bar(stat = "identity") +
  labs(title = "Is a college private or public?",
       subtitle = "Fitting classification accuracy",
       y = "Overall Accuracy") +
  ylim(c(0, 1)) +
  annotate(geom = "label", 
           x = accuracy$model, 
           y = accuracy$accuracy, 
           label = round(accuracy$accuracy, 2))
```

## Z - Challenges

1. Conduct a regression analysis predicting the percent of alumni who donate to the college (`perc_alumni`). How good can your regression model fit this criterion? Which variables seem to be important in predicting it?

2. Conduct a classification analysis predicting whether or not a school is 'hot' -- where a 'hot' school is one that receives at least 10,000 applications (Hint: use the code below to create the `hot` variable).

```{r, eval = FALSE, echo = TRUE}
# Add a new factor criterion 'hot' which indicates whether or not a schol receives at least 10,000 applications

college_train <- college_train %>%
  mutate(hot = factor(Apps >= 10000))
```

3. Did you notice anything strange in your model when doing the previous task? If you used all available predictors you will have gotten a warning that your model did not converge. That can happen if the maximum number of iterations (glm uses an iterative procedure when fitting the model) is reached. The default is a maximum of 25 iterations, see `?glm.control`. To fix it just add the following code in your `train()` function `control = list(maxit = 75)`, and run it again.

4. Now the model should have converged, but there is still another warning occurring: `glm.fit: fitted probabilities numerically 0 or 1 occurred`. This can happen if very strong predictors occur in the dataset (see [Venables & Ripley, 2002](http://www.bagualu.net/wordpress/wp-content/uploads/2015/10/Modern_Applied_Statistics_With_S.pdf), p. 197). If you added all predictors (except again the college names), then this problem occurs because the `Apps` variable, used to create the criterion, was also part of the predictors (plus some other variables that highly correlate with `Apps`). Check the variable correlations (the code below will give you a matrix of bivariate correlations).

```{r, eval = FALSE, echo = TRUE}
# get correlation matrix of numeric variables

cor(college_train[,sapply(college_train, is.numeric)])
```


5. Now fit the model again but only select variables that are not directly related to the number of applications (here several solutions are possible, there is nor clear cut criterion about which variables to include and which to discard).

```{r, eval = FALSE, echo = FALSE}
Private_glm <- train(form = hot ~. - college_name - Apps -Enroll -Accept - F_Undergrad,
                     data = college_train,
                     method = "glm",
                     trControl = ctrl_none,
                     control = list(maxit = 75))
```


