---
title: "Fitting"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
output:
  html_document:
    css: practical.css
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = TRUE, 
                      eval = FALSE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)
```

```{r, echo = FALSE, fig.align = 'center', eval = TRUE, out.width = "70%", fig.cap="Source: www.niche.com"}
knitr::include_graphics("https://www.niche.com/blog/wp-content/uploads/2016/10/best-colleges-in-south-2000px.png")
```

## {.tabset}

### Overview

In this practical you'll practice the basics of fitting three different machine learning models in R

By the end of this practical you will know how to:

1. Fit regression, decision trees, and random forests to training data.
2. Explore each object with generic functions.
3. Evaluate a model's fitting performance.
4. Compare different models fitting performance.

### Datasets

```{r, eval = TRUE, message = FALSE, echo = FALSE}
library(tidyverse)
library(plotly)
library(ggthemes)
```

|File  |Rows | Columns |
|:----|:-----|:------|
|[college_train.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/_sessions/_data/college_train.csv)| 1000 | 21|

### Glossary

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `summary()`|`base`|    Get summary information from an R object| 
| `names()`|`base`|    See the named elements of a list| 
| `LIST$NAME()`|`base`|    Get the named element `NAME` from a list `OBJECT`| 
| `predict(object, newdata)`|`base`|    Predict the criterion values of `newdata` based on `object`|

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`broom`|`install.packages("broom")`|
|`rpart`|`install.packages("rpart")`|
|`FFTrees`|`install.packages("FFTrees")`|
|`partykit`|`install.packages("partykit")`|
|`party`|`install.packages("party")`|
|`randomForest`|`install.packages("randomForest")`|
|`caret`|`install.packages("caret")`|

### Cheatsheet

*None*

### Examples

```{r, eval = FALSE, echo = TRUE}
# Machine learning basics ------------------------------------

# Step 0: Load packages-----------

library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(rpart)        # For rpart()
library(broom)        # For tidy()
library(caret)        # For resamp 
library(partykit)     # For nice decision trees

# Step 1: Load Training data ----------------------

college_train <- read_csv("1_Data/college_train.csv")

# Convert character to factor

college_train <- college_train %>%
  mutate_if(is.character, factor)

# Step 2: Explore training data -----------------------------

View(college_train)

# Our goal is to predict Grad_Rate.
# We will do a log-transformation on Grad_Rate
#  because it is so heavily skewed

# Step 3: Fit models predicting Grad_Rate ------------------

# Regression
Grad_Rate_lm <- glm(formula = Grad_Rate ~ Private + Books + PhD,
                data = college_train)

# Decision Trees
Grad_Rate_rpart <- rpart(formula = Grad_Rate ~ Private + Books + PhD,
                    data = college_train, 
                    control = list(minbucket = 50))

# Step 4: Explore models -------------------------------

# Regression
summary(Grad_Rate_lm)

# Decision Trees
Grad_Rate_rpart
plot(Grad_Rate_rpart)
text(Grad_Rate_rpart)

# Nicer version!
plot(as.party(Grad_Rate_rpart))

# Step 5: Assess fitting accuracy ----------------------------

# Get fitted values
lm_fit <- predict(Grad_Rate_lm)
rpart_fit <- predict(Grad_Rate_rpart)

# Regression Fitting Accuracy
postResample(pred = lm_fit, 
             obs = college_train$Grad_Rate)

# Decision Tree Fitting Accuracy
postResample(pred = rpart_fit, 
             obs = college_train$Grad_Rate)

# Step 6: Plot fitting accuracy -------------------------

# Tidy competition results
competition_results <- tibble(truth = college_train$Grad_Rate,
                              Regression = lm_fit,
                              Decision_Trees = rpart_fit) %>%
                       gather(group, prediction, -truth)

# Plot!
ggplot(data = competition_results,
       aes(x = truth, y = prediction, col = group)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Predicting College Graduation Rates",
       subtitle = "Regression versus decision trees")
```

# Tasks

## A - Setup

1. Open your `baselrbootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data files listed in the `Datasets` section above are in your `1_Data` folder

```{r}
# Done!
```

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `fitting_practical.R` in the `2_Code` folder.  

3. Using `library()` load the set of packages for this practical listed in the packages section above.

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Fitting Practical

library(tidyverse)
library(broom)
library(rpart)
library(partykit)
library(part)
library(randomForest)
library(caret)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(tidyverse)
```

4. For this practical, we'll use a dataset of 388 U.S. College.  The data is stored in `college_train.csv`. Using the following template, load the dataset into R as `college_train`:

```{r, echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE}
college_train <- read_csv(file = "1_Data/college_train.csv")
```

5. Take a look at the first few rows of the dataset by printing it to the console.

```{r}
college_train
```

6. Print the numbers of rows and columns using the `dim()` function

```{r}
dim(college_train)
```

7. Open the dataset in a new window using `View()`

```{r, eval = FALSE}
View(college_train)
```

## B - Walking through the 7 steps

1. Run Steps 0 through 4 in the Examples section above. Run each line of code individually and explore each object you create. Try to understand each of the steps! If you have trouble, ask for help!

2. Which of the three features (Private, Books, PhD) do the regression and decision tree models use? Do they treat the features equally or do some look more important than others?

3. Run Step 5. Look at the results. Which model has the best fitting performance?

## D - Include random forests

1. Random forests are much more complex than regression and decision trees. Try including Random forests in your analyses as a new competitor. You can use the `randomForest()` function from the `randomForest` package to fit your model!

```{r, eval = FALSE, echo = TRUE}
# Just add the following:

# randomForest
Grad_Rate_rf <- randomForest(formula = Grad_Rate ~ Private + Books + PhD,
                             data = college_train)

# Get fitted values
rf_fit <- predict(Grad_Rate_rf)


# fitting acuracy
postResample(pred = rf_fit, 
             obs = college_train$Grad_Rate)
```

2. How does the fitting performance of random forests compare to the other algorithms in training?

```{r}
# rf is better!
```

## E - Include more features

Until now, you've only been predicting Grad_Rate based on 3 features (bedrooms, bathrooms, and floors). Of course, you have access to lots more data to predict housing Grad_Rates! Now it's time to try using more data to predict Grad_Rate.

1. Look closely again at the columns in the `college_train` data. There are two features in the data that you definitely *don't* want to include in your models. Which two are they?

2. Remove those two features from your training data using the following template:

```{r, eval = FALSE}
# Remove two features (columns) from college_train
college_train <- college_train %>%
  select(-XX, -XX)
```


```{r}
# Remove two features (columns) from college_train
college_train <- college_train %>%
  select(-id, -date)
```

3. Re-run your models, but now predict Grad_Rate based on *all* of the features. To do this, use the formula short hand `formula = Grad_Rate ~ .`.

```{r, echo = TRUE}
# Regression example
Grad_Rate_lm <- glm(formula = Grad_Rate ~.,
                data = college_train)

# Same with other models...
```

4. How does the overall fitting and prediction performance of the models compare to when you only used three features? Did each model improve?

```{r}
# Yes each model improves!!
```

## F - Predict a class

Now it's time to do a classification problem. Now, we will try to predict whether or not a school's graduation rate is greater than 90%

```{r}
# Code to create factor
```

```{r}
# Code explaining how to measure accuracy in classification (using caret)
```

...
