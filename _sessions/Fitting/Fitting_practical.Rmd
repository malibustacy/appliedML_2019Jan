---
title: "Fitting"
subtitle: "Model fitting with regression"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
output:
  html_document:
    css: practical.css
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = TRUE, 
                      eval = FALSE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)
```

```{r, echo = FALSE, fig.align = 'center', eval = TRUE, out.width = "70%", fig.cap="Source: wikipedia"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png")
```

## {.tabset}

### Overview

In this practical you'll practice the basics of fitting and exploring regression models in R.

By the end of this practical you will know how to:

1. Fit regression to training data.
2. Explore your object with generic functions.
3. Evaluate its fitting performance using accuracy measures such as MSE and MAE.
4. Explore the effects of adding additional features.

### Datasets

```{r, eval = TRUE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
```

|File  |Rows | Columns |
|:----|:-----|:------|
|[college_train.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/_sessions/_data/college_train.csv)| 1000 | 21|

### Glossary

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `trainControl()`|`caret`|    Define modelling control parameters| 
| `train()`|`caret`|    Train a model|
| `predict(object, newdata)`|`base`|    Predict the criterion values of `newdata` based on `object`|
| `postResample()`|`caret`|   Calculate aggregate model performance in regression tasks|
| `confusionMatrix()`|`caret`|   Calculate aggregate model performance in classification tasks| 

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`caret`|`install.packages("caret")`|

### Cheatsheet

<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf">
  <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet-200x155@2x.png" alt="Trulli" style="width:70%">
  <figcaption>hhttps://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption></a>
</figure>

### Examples

```{r, eval = FALSE, echo = TRUE}
# Fitting and evaluating a regression model ------------------------------------

# Step 0: Load packages-----------

library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 

# Step 1: Load and Clean, and Explore Training data ----------------------

# I'll use the mpg dataset from the dplyr package in this example
#  no need to load an external dataset

data_train <- read_csv("1_Data/mpg_train.csv")

# Convert all characters to factor
#  Some ML models require factors

data_train <- data_train %>%
  mutate_if(is.character, factor)

# Explore training data

data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Step 2: Define training control parameters -------------

# In this case, I will set method = "none" to fit to 
#  the entire dataset without any fancy methods

train_control <- trainControl(method = "none") 

# Step 3: Train model: -----------------------------
#   Criterion: hwy
#   Features: year, cyl, displ, trans

# Regression

hwy_glm <- train(form = hwy ~ year + cyl + displ + trans,
                 data = data_train,
                 method = "glm",
                 trControl = train_control)

# Look at summary information
summary(hwy_glm)

# Step 4: Access fit ------------------------------

# Save fitted values
glm_fit <- predict(hwy_glm)

# Define data_train$hwy as the true criterion
criterion <- data_train$hwy

# Regression Fitting Accuracy
postResample(pred = glm_fit, 
             obs = criterion)

#     RMSE Rsquared      MAE 
# 3.825010 0.622349 2.767995 

# On average, the model fits are 2.8 away from the true
#  criterion values

# Step 5: Visualise Accuracy -------------------------

# Tidy competition results
accuracy <- tibble(criterion = criterion,
                   Regression = glm_fit) %>%
               gather(model, prediction, -criterion) %>%
  
  # Add error measures
  mutate(se = prediction - criterion,
         ae = abs(prediction - criterion))


# Calculate summaries
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of criterion versus predictions

ggplot(data = accuracy,
       aes(x = criterion, y = prediction, col = model)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Predicting mpg$hwy",
       subtitle = "Black line indicates perfect performance")

# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Distributions of Fitting Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))
```

# Tasks

## A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data file(s) listed in the `Datasets` section above are in your `1_Data` folder

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `Fitting_practical.R` in the `2_Code` folder.  
3. Using `library()` load the set of packages for this practical listed in the packages section above.

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Fitting Practical

library(tidyverse)
library(caret)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(tidyverse)
```

4. For this practical, we'll use a dataset of 388 U.S. Colleges.The data is stored in `college_train.csv`. Using the following template, load the dataset into R as `college_train`:

```{r, echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE}
# Load in college_train.csv data as college_train

college_train <- read_csv(file = "1_Data/college_train.csv")
```

5. Take a look at the first few rows of the dataset by printing it to the console.

```{r}
college_train
```

6. Print the numbers of rows and columns using the `dim()` function

```{r}
# Print number of rows and columns of college_train

dim(college_train)
```

7. Look at the names of the dataframe with the `names()` function

```{r}
names(college_train)
```

8. Open the dataset in a new window using `View()`. How does it look?

```{r, eval = FALSE}
View(college_train)
```

9. We need to do a little bit of data cleaning before starting. Specifically, we need to convert all character columns to factors: Do this by running the following code:

```{r}
# Convert character to factor and remove college_name

college_train <- college_train %>%
          mutate_if(is.character, factor)
```

## B - Determine sampling procedure

In `caret`, we always need to define computational nuances of training using the `trainControl()` function. Because we're learning the basics of fitting, we'll set `method = "none"` (Note that you would almost never do this for a real prediction task, you'll see why later!)

```{r}
# Set training resampling method to "none" to keep everything super simple
#   for demonstration purposes

ctrl_none <- trainControl(method = "none") 
```

## C - Fit a regression model

1. Using the code below, fit a regression model predicting graduation rate (`Grad_Rate`) as a function of one feature `PhD`, the percent of faculty with Ph.Ds). Save the result as an object `Grad_Rate_glm`

```{r}
# Grad_Rate_glm: Regression Model
#   Criterion: Grad_Rate
#   Features: PhD

Grad_Rate_glm <- train(form = Grad_Rate ~ PhD,
                       data = college_train,
                       method = "glm",
                       trControl = ctrl_none)
```


2. Explore the model using the `summary()` function.

```{r}
# Show summary information from the regression model

summary(Grad_Rate_glm)
```

3. Look at the results, how do you interpret the regression coefficients? What is the general relationship between PhD and graduation rates? Does this make sense?

4. Now it's time to save the model fits! Do this by running the following code to save the fitted values as `glm_fit`

```{r}
# Get fitted values from the model and save as glm_fit

glm_fit <- predict(Grad_Rate_glm)
```

5. Print your `glm_fit` object - what are these values? Do they look reasonable? That is, are they in the range of what you expect the criterion to be?

```{r}
# Print glm_fit

glm_fit
```

## D - Evaluate accuracy

1. Now it's time to compare your model fits to the true values, we'll start by defining the vector `criterion` as the graduation rates.

```{r}
# Define criterion as Grad_Rate

criterion <- college_train$Grad_Rate
```

2. Let's quantify our model's fitting results. To do this, we'll use the `postResample()` function, with the fitted values as the prediction, and the criterion as the observed values:

```{r}
# Regression Fitting Accuracy

postResample(pred = glm_fit,   # Fitted values 
             obs = criterion)  # criterion values
```

3. You'll see three values here, the easiest to understand is MAE which stands for "Mean Absolute Error" -- in other words, "on average how far are the predictions from the true values?" A value of 0 means perfect prediction, so small values are good! How do you interpret these results?

4. Now we're ready to do some plotting. But first, we need to re-organise the data a bit. We'll create two dataframes
  - `accuracy`: Raw absolute errors
  - `accuracy_agg` Aggregate (i.e.; mean) absolute errors

```{r}
# accuracy: a dataframe of raw absolute errors
accuracy <- tibble(criterion = criterion,
                   Regression = glm_fit) %>%
                gather(model, prediction, -criterion) %>%
  
  # Add error measures
  mutate(ae = abs(prediction - criterion))

# accuracy_agg: Dataframe of aggregate errors
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)
```

5. Print the `accuracy` and `accuracy_agg` objects to see how they look.

6. Using the code below, create a scatterplot showing the relationship between the true criterion values and the model fits

```{r}
# Plot A) Scatterplot of criterion versus predictions

ggplot(data = accuracy,
       aes(x = criterion, y = prediction)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Regression: One Feature",
       subtitle = "Line indicates perfect performance",
       x = "True Graduation Rates",
       y = "Fitted Graduation Rates") +
  xlim(0, 120) + 
  ylim(0, 120)
```

7. Look at the plot, how do you interpret this? Do you think the model did well or not in fitting graduation rates?

8. Let's create a new violin plot showing the distribution of absolute errors of the model:

```{r}
# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Distributions of Fitting Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))
```

9. What does the plot show you about the model fits? On average, how far away were the model fits from the true values?


## F - Add more features

So far we have only used one feature (`PhD`), to predict `Grad_Rate`. Let's try again, but now we'll use a total of four features:
- `PhD`: The percent of faculty with a PhD
- `Room.Board`: Room and board costs
- `Terminal`: Percent of faculty with a terminal degree
- `S.F.Ratio`: Student to faculty ratio

1. Using the same steps as above, create a regression model `Grad_Rate_glm` which predicts `Grad_Rate` using all 4 features (you can also call it something else if you want to save your original model!).

2. Explore your model using `summary()`, which features seem to be important?

3. What is the Mean Absolute Error (MAE) of your new model that uses 4 features? How does this compare to your previous model that only used 1 feature?

4 (Optional). Create a scatterplot showing the relationship between your new model fits and the true values. How does this plot compare to your previous one?

5 (Optional). Create a violin plot showing the distribution of absolute errors. How does this compare to your previous one?

## F - Use (almost) all features

Alright now it's time to use all features available!

1. Before we run the model, we need to make sure there aren't any features we *don't* want to include. Look at the `college_train` dataframe again -- do you see the one feature that we definitely don't want to include in the model?

2. Using the same steps as above, create a regression model `glm_fit` which predicts `Grad_Rate` using all features *except* for that one we don't want (remember, use Y ~ . - XXX to remove a feature XXX)

```{r, echo = TRUE}
Grad_Rate_glm <- train(form = Grad_Rate ~ . - XXX,   # Get rid of that one feature!
                       data = college_train,
                       method = "glm",
                       trControl = ctrl_none)
```

3. Explore your model using `summary()`, which features seem to be important?

4. What is the Mean Absolute Error (MAE) of your new model that uses 6 features? How does this compare to your previous model that only used 1 feature?

5 (Optional). Create a scatterplot showing the relationship between your new model fits and the true values. How does this plot compare to your previous one?

6 (Optional). Create a violin plot showing the distribution of absolute errors. How does this compare to your previous one?

# Classification

## G - Make sure your criterion is a factor!

Now it's time to do a classification task! Recall that in a classification task, we are predicting a category, not a continuous number. In this task, we'll predict whether or not a college is Private or Public, this is stored as the variable `college_train$Private`

1. In order to do classification training with `caret`, all you need to do is make sure that the criterion is coded as a factor. To see that is coded as a factor, you can look at its class as follows:

```{r}
# Look at the class of the variable Private, should be a factor!

class(college_train$Private)
```

2. Now, we'll save the Private column as a new object called `criterion`

```{r}
# Define criterion as college_train$Private

criterion <- college_train$Private
```

## H - Fit a classification model

1. Using `train()`, create `Private_glm()`, a regression model predicting the variable `Private` (Don't forget to ignore that one variable we don't want to use!)

```{r, echo = FALSE}
# Fit regression model predicting private
Private_glm <- train(form = Private ~ . -college_name,
                     data = college_train,
                     method = "glm",
                     trControl = ctrl_none)
```

2. Explore the model using the `summary()` function.

```{r}
# Show summary information from the regression model

summary(Private_glm)
```

3. Look at the results, how do you interpret the regression coefficients? Which features seem important in predicting whether a school is private or not?

## I - Access classification model accuracy

1. Now it's time to save the model fits! Do this by running the following code to save the fitted values as `glm_fit`

```{r}
# Get fitted values

glm_fit <- predict(Private_glm)
```

2. Print your `glm_fit` object - what are these values? Do they look reasonable?

```{r}
# Print glm_fit

glm_fit
```

3. Now it's time to calculate model accuracy, to do this, we will use a new function called `confusionMatrix()`. This function compares model predictions to a 'reference' (in our case, the criterion, and returns several summary statistics). In the code below, we'll use `glm_fit` as the model predictions, and our already defined `criterion` vector as the reference (aka, truth)

```{r}
# Show accuracy of glm_fit versus the true values

confusionMatrix(data = glm_fit,        # This is the prediction!
                reference = criterion) # This is the truth!
```

4. Look at the results, what is the overall accuracy of the model? How do you interpret this?

5. What is the sensitivity? How do you interpret this number?

6. What is the specificity? How do you interpret this number?

7. To visualise the accuracy of your classification models, use the following code to create a barplot:

```{r}
# Get overall accuracy from regression model
glm_accuracy <- confusionMatrix(data =  glm_fit,  
                                reference = criterion)$overall[1]

# Combine results into one table
accuracy <- tibble(Regression = glm_accuracy) %>%
              gather(model, accuracy)


# Plot the results!
ggplot(accuracy, aes(x = model, y = accuracy, fill = model)) + 
  geom_bar(stat = "identity") +
  labs(title = "Is a college private or public?",
       subtitle = "Fitting classification accuracy",
       y = "Overall Accuracy") +
  ylim(c(0, 1)) +
  annotate(geom = "label", 
           x = accuracy$model, 
           y = accuracy$accuracy, 
           label = round(accuracy$accuracy, 2))
```

## Z - Challenges

1. Conduct a regression analysis predicting the percent of alumni who donate to the college (`perc.alumni`). How good can your regression model fit this criterion? Which variables seem to be important in predicting it?

2. Conduct a classification analysis predicting whether or not a school is 'hot' -- where a 'hot' school is one that receives at least 10,000 applications (Hint: use the code below to create the `hot` variable). How good can your regression model predict whether or not a school is 'hot'?

```{r, eval = FALSE, echo = TRUE}
# Add a new factor criterion 'hot' which indicates whether or not a schol receives at least 10,000 applications

college_train <- college_train %>%
  mutate(hot = factor(Apps >= 10000))
```

