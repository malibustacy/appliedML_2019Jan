---
title: "Model Fitting"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
date: "January 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer"><span>
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">Applied Machine Learning with R, January 2019</font></a>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">www.therbootcamp.com</font></a>
</span></div> 

---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)


# Get color palette functions

source("https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_materials/code/baselrbootcamp_palettes.R")
```

```{r, echo = FALSE ,message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, 
                      message = FALSE, warning = FALSE, fig.align = 'center', dpi = 200)
library(tidyverse)
library(baselers)
library(ggthemes)
library(ggpubr)
library(caret)
library(ISLR)
```


.pull-left4[

# Where we are at

- Have a business question
    - How can I predict loan default?

- Have data relevant to that question
    - Records from 300 historical customers

- Data is cleaned and in a tidy, rectangular format
    - Database, .csv,

## What's next?

<high>Select</high> and <high>train</high> model(s) depending on the <high>type of task</high>

]

.pull-right6[
<br><br><br>
```{r, echo = FALSE, fig.align = 'center', out.width = "100%", fig.cap = "Source: Medium.com"}
knitr::include_graphics("https://cdn-images-1.medium.com/max/1600/1*_QGyIwpgq831xI54cIe_GQ.jpeg")
```

]


---

.pull-left45[

# What type of task do you have?

There are many types of of ML tasks.

In this course, we will focus on 2 of the most popular

|Type|Description | Example |
|:----|:---- |:----|
|<high>Regression</high> (supervised)|Predicting a number|Stock prices|
|<high>Classification</high> (supervised)|Predicting a category, like whether|Whether someone will purchase a product or not|

]

.pull-right5[
<br><br><br>

```{r, echo = FALSE, fig.align = 'center', out.width = "100%"}
knitr::include_graphics("https://github.com/therbootcamp/appliedML_2019Jan/blob/master/_sessions/Fitting/image/class_v_regression.jpg?raw=true")
```

]

---

.pull-left45[

# What ML models are there?

There are *thousands* of machine learning models

In this course, you will learn 3 of the most popular:

|Model|Description|
|:----|:---- |
|Regression|A weighted linear combination of features and weights|
|Decision Tree|A series of hierarchical 'yes/no' decisions|
|Random Forests|Combination of many decision trees|

*In the Models session, you'll learn about many more!*

]

.pull-right5[

```{r, echo = FALSE, fig.align = 'center', out.width = "100%"}
knitr::include_graphics("https://github.com/therbootcamp/appliedML_2019Jan/blob/master/_sessions/Fitting/image/three_models_vert.jpg?raw=true")
```

]

---
class: center, middle

## Once you have a model you need to "Fit" (aka, "Train") it to data

```{r, echo = FALSE, out.width = "50%", fig.cap = "<font size = 4>James et al., Introduction to SL</font>"}
knitr::include_graphics("image/regression_ISLR.jpg")
```

## What does that mean?


---


.pull-left5[

# What does "Fitting" mean?

For any model class, there is no "One" single model.

> How many possible Regression models are there?


Any machine learning model can be 'fit' (aka 'trained') on a specific dataset of interest.


Fitting means finding the "best" version of a model for a specific dataset.

> "Let me represent the data in the best way I can given how I work"<br>
>~ Model during fitting

The "best" model is usually defined as a combination of <high>accuracy</high> (higher better!) and <high>complexity</high> (simpler is better!)


]

.pull-right45[

```{r, echo = FALSE}
set.seed(101)
x <- rnorm(20)
y <- .7 * x + rnorm(20) + 10

data <- data.frame(x, y)

mod <- lm(y ~ x, data = data)

library(tidyverse)

three_mod <- ggplot(data, aes(x = x, y = y)) + geom_point() +
    theme_minimal() +
    scale_color_baselrbootcamp()

  # geom_abline(slope = 2, intercept = 8, col = "blue", size = 1) +
  # geom_abline(slope = .2, intercept = 11, col = "red", size = 1) +
  # geom_abline(slope = mod$coefficients[2], intercept = mod$coefficients[1], col = "green", size = 1) +

```


<br><br>
### How do I fit a model to these data??

```{r, echo = FALSE, fig.width = 3, fig.height = 3, dpi = 200, out.width = "80%"}
three_mod
```


]




```{r, echo = FALSE}
set.seed(102)
x <- rnorm(10)
y <- .7 * x + rnorm(10, sd = .3) + 2

data <- data.frame(x, y)

mod <- lm(y ~ x, data = data)

great_intercept <- mod$coefficients[1]
great_slope <- mod$coefficients[2]

bad_intercept <- 3.5
bad_slope <- -.5

x0 = x
x1 = x
y0 = y
y1 = great_intercept + great_slope * x

dat_great <- data.frame(x0, x1, y0, y1)

x0 = x
x1 = x
y0 = y
y1 = bad_intercept + bad_slope * x

dat_bad <- data.frame(x0, x1, y0, y1)

library(tidyverse)

raw <- ggplot(dat_great, aes(x = x0, y = y0)) + geom_point(col = baselrbootcamp_cols("grey"), size = 2) +
  theme_minimal() +
  xlim(c(-2, 3)) +
  ylim(c(0, 5)) +
  labs(title = "Raw Data", 
       x = "Feature", y = "Criterion")

great_raw <- ggplot(dat_great, aes(x = x0, y = y0)) + geom_point(col = baselrbootcamp_cols("grey"), size = 2) +
  geom_abline(slope = great_slope, intercept = great_intercept, size = .5, linetype = 3) +
  theme_minimal() +
  xlim(c(-2, 3)) +
  ylim(c(0, 5)) +
  labs(title = "Model B", 
              subtitle = paste0("B0 = ", round(great_intercept, 2), ", B1 = ", round(great_slope, 2)),
       caption = paste("Mean Squared Error (MSE) = ?"),

       x = "Feature", y = "Criterion")

bad_raw <- ggplot(dat_bad, aes(x = x0, y = y0)) + geom_point(col = baselrbootcamp_cols("grey")) +
  geom_abline(slope = bad_slope, intercept = bad_intercept, size = .5, linetype = 3) +
  theme_minimal() +
  xlim(c(-2, 3)) +
  ylim(c(0, 5)) +
   labs(title = "Model A", 
        subtitle = paste0("B0 = ", round(bad_intercept, 2), ", B1 = ", round(bad_slope, 2)),
       caption = paste("Mean Squared Error (MSE) = ?"),
       x = "Feature", y = "Criterion")

great_err <- great_raw + 
  geom_linerange(data = dat_great, aes(x = x0, ymin = y0, ymax = y1), col = baselrbootcamp_cols("magenta")) +
  geom_point(data = dat_great, aes(x = x0, y = y1, size = 2), col = baselrbootcamp_cols("green"), pch = "X", size = 4) +
    labs(title = "Model B - Better", 
       caption = paste("Mean Squared Error (MSE) = ", round(mean((dat_great$y1 - dat_great$y0) ^ 2), 2)),
       x = "Feature", y = "Criterion")

bad_err <- bad_raw +
    geom_linerange(data = dat_bad, aes(x = x0, ymin = y0, ymax = y1), col = baselrbootcamp_cols("magenta")) +
    geom_point(data = dat_bad, aes(x = x0, y = y1, size = 2), col = baselrbootcamp_cols("green"), pch = "X", size = 4) +
   labs(title = "Model A - Worse", 
       caption = paste("Mean Squared Error (MSE) = ", round(mean((dat_bad$y1 - dat_bad$y0) ^ 2), 2)),
       x = "Feature", y = "Criterion")
```



---

.pull-left45[
<br><br><br>
# Defining Accuracy (or Error)

To train (fit) a model to a dataset, we need to <high>mathematically define Accuracy</high>

Alternatively, we can define a model's <high>Error</high>

There is <high>no 'correct'</high> definition of error, it depends on <high>what's important to you</high> as the decision maker!

Once accuracy (or error) is defined, a model can be trained to maximize (or minimize) it!

The model that minimizes error (or maximizes accuracy) is the final <high>Training model</high>

]

.pull-right45[

<br><br>
### How do I fit a model to these data??

```{r, echo = FALSE, fig.width = 3, fig.height = 3, dpi = 200, out.width = "80%"}
three_mod
```
]

---

# Which of these models is better? Why?

```{r, echo = FALSE, fig.width = 6, fig.height = 3, dpi = 200, out.width = "90%"}
ggarrange(bad_raw, great_raw, ncol = 2, nrow = 1)
```


---

# Which of these models is better? Why?

```{r, echo = FALSE, fig.width = 6, fig.height = 3, dpi = 200, out.width = "90%"}
ggarrange(bad_err, great_err, ncol = 2, nrow = 1)
```


---

.pull-left45[

# Regression Error

### MAE: Mean Absolute Error

$$\large MSE = \frac{1}{n}\sum_{i=1}^{n} \lvert Prediction_{i} - Truth_{i} \rvert$$

> On average, how far are predictions away from true values?

### MSE: Mean Squared Error

$$\large MSE = \frac{1}{n}\sum_{i=1}^{n}(Prediction_{i} - Truth_{i})^{2}$$
> On average, how far are predictions away from true values (squared!)?


]

.pull-right5[

```{r, fig.width = 3, fig.height = 3, echo = FALSE}
bad_err +
  labs(subtitle = "Red lines are (absolute) errors", title = "")
```

]






---

.pull-left45[

# Classification Accuracy

Classification accuracy measures all come from the <high> "confusion matrix"</high>

The confusion matrix is a cross tabulation table showing predictions versus true classes.

### Confusion Matrix

|    |      Y is Positive      | Y is Negative |
|----------|:-------------:|:------:|
| Predict <br>"Positive"| <font color = "green">TP<br> True Positive</font>| <font color = "red">FP <br> False Positive</font> |
| Predict <br>"Negative"|    <font color = "red">FN<br> False Negative</font>   |  <font color = "green">TN <br> True Negative</font>|

Green cells are <font color = "green">correct decisions</font> while Red cells are <font color = "red">incorrect decisions</font>


]

.pull-right5[

### Data

||X1|X2|X3|Prediction|Truth|Outcome|
|:---|:----|:----|:----|:----|:----|:----|
|1|.|.|.|"Default"|Default|<font color = "green">TP</font>|
|2|.|.|.|"Default"|Default|<font color = "green">TP</font>|
|3|.|.|.|"Repay"|Repay|<font color = "green">TN</font>|
|4|.|.|.|"Default"|Repay|<font color = "red">FP</font>|
|5|.|.|.|"Repay"|Default|<font color = "red">FN</font>|
|6|.|.|.|"Default"|Default|<font color = "green">TP</font>|
|7|.|.|.|"Repay"|Repay|<font color = "green">TN</font>|

### Confusion Matrix

|    |      True Default      | True Repay  |
|----------|:-------------:|:------:|
| Predict <br>"Default"| <font color = "green">3</font>| <font color = "red">1 </font> |
| Predict <br>"Repay"|    <font color = "red">1</font>   |  <font color = "green"> 2</font>|

]

---

.pull-left45[

# Classification Accuracy

Classification accuracy measures all come from the <high> "confusion matrix"</high>

The confusion matrix is a cross tabulation table showing predictions versus true classes.

### Confusion Matrix

|    |      Y is Positive      | Y is Negative |
|----------|:-------------:|:------:|
| Predict <br>"Positive"| <font color = "green">TP<br> True Positive</font>| <font color = "red">FP <br> False Positive</font> |
| Predict <br>"Negative"|    <font color = "red">FN<br> False Negative</font>   |  <font color = "green">TN <br> True Negative</font>|

Green cells are <font color = "green">correct decisions</font> while Red cells are <font color = "red">incorrect decisions</font>


]

.pull-right5[


### Overall Accuracy

> What percent of my predictions are correct?

$$\large Overall \; Accuracy = \frac{TP + TN}{ TP + TN + FN + FP}$$

### Sensitivity

> <i>Of the truly Positive cases</i>, what percent of predictions are correct?


$$\large Sensitivity = \frac{TP}{ TP +FN }$$
### Specificity

> <i>Of the truly Negative cases</i>, what percent of predictions are correct?


$$\large Specificity = \frac{TN}{ TN + FP }$$


]



---

.pull-left45[

# Classification Accuracy

### Example: Loan default

Imagine we use a model (e.g. a decision tree) to predict whether or not each of 7 customers will default on their loan.

After the loan period is over, we obtain the final confusion matrix comparing our predictions to the truth:

### Confusion Matrix

|    |      True Default      | True Repay  |
|----------|:-------------:|:------:|
| Predict <br>"Default"| <font color = "green">TP <br>3</font>| <font color = "red">FP <br>1 </font> |
| Predict <br>"Repay"|    <font color = "red">FN<br>1</font>   |  <font color = "green">TN <br> 2</font>|

]

.pull-right5[


### Overall Accuracy

Across all customers, our model has an accuracy of 71%

$$\large Overall \; Accuracy = \frac{3 + 2}{3 + 2 + 1 + 1} = 0.71$$

### Sensitivity

Our model is 75% accurate in catching true defaults

$$\large Sensitivity = \frac{3}{3 + 4} = .75$$
### Specificity

Our model is 67% accurate in catching true repayments


$$\large Specificity = \frac{2}{ 2 + 1 }= 0.67$$


]


---

.pull-left45[

# Ready to fit!

Now we're ready to fit models to data!

In this course will cover three commonly used models, Regression, Decision Trees, and Random Forest

These models can be used in both regression and classification tasks.

As you'll see, they differ in complexity (interpretability, computational demands)

|Model|Complexity|
|:----|:---- |
|Regression|<font color = "orange">Medium</font>|
|Decision Tree|<font color = "green">Low</font> (usually)|
|Random Forests|<font color = "red">High</font>|

]

.pull-right5[

```{r, echo = FALSE, fig.align = 'center', out.width = "100%"}
knitr::include_graphics("https://github.com/therbootcamp/appliedML_2019Jan/blob/master/_sessions/Fitting/image/three_models_vert.jpg?raw=true")
```

]

---
class: middle, center



# Regression

Undoubtedly the most widely used machine learning algorithm

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("https://www.rmurphyknives.com/store/media/Cooking/ChefsSelect/Chef8/CH8CIIHO2500x1667.jpg")
```

---

.pull-left4[

# Regression

$$\large \hat{Y} =  \beta_{0} + X1 \times \beta_{X1} + X2 \times \beta_{X2} + ...$$

```{r, echo = FALSE, out.width = "100%", fig.cap = "<font size = 4>James et al., Introduction to SL</font>"}
knitr::include_graphics("image/regression_ISLR.jpg")
```



]

.pull-right55[

<br><br>

### Interpretation

In [regression](https://en.wikipedia.org/wiki/Regression_analysis), the criterion Y is modeled as the <high>sum</high> of <high>predictors times weights</high> $\beta_{1}$, $\beta_{2}$</high>.

Often called a 'weighted additive model'

$$\LARGE \hat{Y} =  \beta_{0} + X1 \times \beta_{X1} + X2 \times \beta_{X2} + ...$$

Each beta weight $\beta$ can be interpreted as:


> As the value of X increases by 1, how does the criterion Y change?



]

---

.pull-left4[

# Regression

In [regression](https://en.wikipedia.org/wiki/Regression_analysis), the criterion Y is modeled as the <high>sum</high> of <high>predictors times weights</high> $\beta_{1}$, $\beta_{2}$</high>.

$$\hat{Y} =  \beta_{0} + X1 \times \beta_{X1} + X2 \times \beta_{X2} + ...$$
```{r, echo = FALSE, out.width = "100%", fig.cap = "<font size = 4>James et al., Introduction to SL</font>"}
knitr::include_graphics("image/regression_ISLR.jpg")
```

]

.pull-right55[

## Sales Example

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("image/Carseats_ss.jpg")
```


#### Regression Model

$$\large Sales =  \beta_{0} + CompPrice \times \beta_{CompPrice} + Income \times \beta_{Income} + ...$$
#### Example coefficients

|Parameter| $\beta_{0}$| $\beta_{CompPrice}$ | $\beta_{Income}$ | $\beta_{Advertising}$ |
|:-----|:-----|:-----|:----|:----|
| Estimate| 10 | 5.4 | 1.3 |2.4 |

]



---
class: center,  middle

<br><br>

# Let's fit models with caret!

```{r, echo = FALSE, out.width = "70%"}
knitr::include_graphics("https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/Caret-package-in-R.png")
```


```{r, echo = FALSE}
library(caret)
data(cars)
```


---

.pull-left55[

# `caret`

Main caret fitting functions:

| Function| Purpose|
|--------|----------|
| [trainControl()](http://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning) | Determine how training (in general) will be done|
| [train()](http://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning) | Specify a model and find *best* parameters|
| predict() | Predict values (either fitted values or predictions for new data)|
| [postResample()](http://topepo.github.io/caret/measuring-performance.html) | Evaluate model performance (fitting or prediction)|

]

.pull-right4[


```{r, eval = FALSE}
# Step 1: Load data
#   read_csv()

data_train <- read_csv(...)

# Step 2: Define control parameters
#   trainControl()

ctrl <- trainControl(...) 

# Step 3: Train and explore model
#   train()

mod <- train(...)
summary(mod)
mod

# Step 4: Assess fit
#   predict(), postResample()

fit <- predict()
postResample(fit, truth)

# Step 5: Visualise results

ggplot(...)
```



<!-- Caret documentation: [http://topepo.github.io/caret/](http://topepo.github.io/caret/) -->

<!-- <iframe src="http://topepo.github.io/caret/" height="480px" width = "500px"></iframe> -->

]

---

.pull-left55[

## trainControl()

Use `trainControl()` to define how `caret` should, generally, <high>select the best parameters</high> for an ML model.

Here you can tell `caret` to do things like repeated <high>cross validation</high> (which we will learn about later).

|Argument|Description|
|:-----|:----|
|`method`|How should fitting be done?|

For now, we'll set `method = "none"` to keep things simple. 

This means "Fit the model without advanced parameter tuning"

```{r}
# Fit the model without any 
#  advanced parameter tuning methods

ctrl <- trainControl(method = "none")
```

]

.pull-right4[
<br><br>
Help menu for `?trainControl()`

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("image/traincontrol_help.jpg")
```

]

---


.pull-left4[

## `train()`

`train()` is the workhorse fitting function of `caret`.

With just this one function, you can <high>fit any of 200+ models</high>...

...just by changing the <high>method</high> argument!

### train() Arguments


|Argument|Description|
|:-----|:----|
|`form`|Formula specifying criterion|
|`data`|Training data|
|`method`| Model|
|`trControl`| Control parameters|

]


.pull-right55[

<br><br><br>
### Train a Regression model

Regression: `method = "glm"`

```{r, echo = TRUE, out.width = "90%", eval = TRUE, warning = FALSE}
# Fit a regression model predicting Price

mod <- train(form = Price ~ .,  # Formula
             data = cars,       # Training data
             method = "glm",    # Regression
             trControl = ctrl)  # Control Parameters
```


]


---


.pull-left4[

## `train()`

`train()` is the workhorse fitting function of `caret`.

With just this one function, you can <high>fit any of 200+ models</high>...

...just by changing the <high>method</high> argument!

### train() Arguments


|Argument|Description|
|:-----|:----|
|`form`|Formula specifying criterion|
|`data`|Training data|
|`method`| Model|
|`trControl`| Control parameters|
]


.pull-right55[

<br><br><br>

### Train a Random Forest model

Random Forest: `method = "rf"`

```{r, echo = TRUE, out.width = "90%", eval = TRUE, warning = FALSE}
# Fit a Random Forest model predicting Price

mod <- train(form = Price ~ .,  # Formula
             data = cars,       # Training data
             method = "rf",     # Random Forests
             trControl = ctrl)  # Control Parameters
```

]


---

.pull-left4[

## `train()`

`train()` is the workhorse fitting function of `caret`.

With just this one function, you can <high>fit any of 200+ models</high>...

...just by changing the <high>method</high> argument!

### train() Arguments


|Argument|Description|
|:-----|:----|
|`form`|Formula specifying criterion|
|`data`|Training data|
|`method`| Model|
|`trControl`| Control parameters|

]


.pull-right55[

Find all 280+ models [here](http://topepo.github.io/caret/available-models.html)

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("image/caret_models_ss.jpg")
```


]

---

.pull-left4[

## `train()`

Make sure your criterion is the correct class for your type of modelling task

- Numeric criterion = Regression Task
- Factor criterion = Classification Task

```{r, eval = FALSE}
# My training data
Loans
```

```{r, echo = FALSE}
Loans <- tibble::tibble(Default = c(0, 1, 0, 1, 1),
              Age = c(45, 36, 76, 25, 36),
             Gender = c("M", "F", "F", "M", "F"),
             Cards = c(3, 2, 5, 2, 3),
             Education = c(11, 14, 12, 17, 12)
             )

Loans
```

See that the column Default is 0's and 1's, but is coded as numeric.

]

.pull-right55[

<br><br>
This code will think that Default is a continuous number, not a category (probably not what you want)

```{r, eval = FALSE}
# Will be a regression task if Default is numeric

mod <- train(form = Default ~ .,
             data = Loans,
             method = "glm")
```

<font color='red' size = 3>Warning messages:...Are you sure you wanted to do regression?</font>

Use `factor()` to <high>convert your criterion</high> to a factor, now you are doing classification!

```{r, eval = FALSE}
# Will be a classification task

mod <- train(form = factor(Default) ~ .,
             data = Loans,
             method = "glm")
```

]

---

.pull-left5[

## `predict()`

The `predict()` function is a generic function in R for returning predictions from a model.

Put your model object as the first argument. If you don't specify a new dataset with `newdata`, the function returns *fitted values from training*


```{r}
# Get fitted values
glm_fits <- predict(object = mod)
```

The result is a vector

```{r}
# Result is a vector of fits
glm_fits[1:5]
```

You can use this vector to compare the model fits to the true values (see plot to right)

]

.pull-right45[

### Plot of fits versus Truth

If the model was perfect, all points would be on diagonal

*Code Hidden*


```{r, fig.width = 4, fig.height = 3, fig.align = 'center', echo = FALSE}
# Plot fits versus truth
tibble(fits = glm_fits,
       truth = cars$Price) %>%
ggplot(aes(x = fits, y = truth)) + 
  geom_abline(slope = 1, intercept = 0) +
  geom_point(alpha = .1) +
  theme_bw()
```


]

---

.pull-left5[

## `postResample()`

To calculate aggregate model performance, use `postResample()`

|Argument|Description|
|:----|:----|
|pred|Model predictions (or fits)|
|obs|The observed (true) values|

```{r}
# Assess performance with postResample()

postResample(pred = glm_fits,   # Predictions 
             obs = cars$Price)  # True values
```

```{r, echo = FALSE, eval = TRUE}
x <- postResample(pred = glm_fits, 
             obs = cars$Price)
```

Here, this tells us that the Mean Absolute Error (MAE) of the model in fitting was `r round(x[3], 2)`

]

.pull-right45[

### Plot of fits versus Truth

Red lines indicate absolute error(s)

*Code Hidden*

```{r, fig.width = 4, fig.height = 3, fig.align = 'center', echo = FALSE}
tibble(fits = glm_fits,
       truth = cars$Price) %>%
ggplot(aes(x = fits, y = truth)) + 
    geom_point(alpha = .2) +
geom_linerange(aes(x = fits, ymin = fits, ymax = truth), col = baselrbootcamp_cols("magenta")) +
  geom_abline(slope = 1, intercept = 0) +
  theme_bw()
```

]


---





---

class: middle, center

<h1><a href=https://therbootcamp.github.io/appliedML_2019Jan/_sessions/Fitting/Fitting_practical.html>Practical</a></h1>


---

.pull-left4[

# Decision Trees

In [decision trees](https://en.wikipedia.org/wiki/Decision_tree), the criterion is modeled as a <high>sequence of logical YES or NO questions</high>.
<br><br>

<p align="center">
  <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/defaulttree.png" height="250px">
</p>

]

.pull-right55[

### Fitting

Goal: Maximize accuracy by define splits that maximize *Node Purity*

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("image/tree_purity_example.png")
```

### R Method

| Variant | Description| Caret |
|:-----|:------|:---|
|rpart |Recursive Partitioning| `"rpart"`|

]

---

.pull-left4[

# Random Forest

In [Random Forest](https://en.wikipedia.org/wiki/Random_forest), the criterion is models as the <high>aggregate prediction of a large number of decision trees</high> each based on different features.
<br>

<p align="center">
  <img src="https://raw.githubusercontent.com/therbootcamp/Erfurt_2018June/master/_sessions/_image/randomforest_diagram.png" height="285px"><br>
  <a href="https://medium.com/@williamkoehrsen">Source</a>
</p>

]

.pull-right55[

### Fitting

Goal: Create a large set of diverse trees that can be aggregated into one *Wisdom of Crowds* judgment

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("image/tree_crowd.png")
```

### R Method

| Variant | Description| Caret |
|:-----|:------|:---|
|Random Forests |"Classic" random forests| `"rf"`|

]

