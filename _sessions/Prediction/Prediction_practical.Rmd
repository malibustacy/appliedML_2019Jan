---
title: "Prediction"
subtitle: "With regression, decision trees, and random forests"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
output:
  html_document:
    css: practical.css
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = TRUE, 
                      eval = FALSE, 
                      warning = FALSE,
                      message = FALSE)

options(digits = 3)
```

```{r, echo = FALSE, fig.align = 'center', eval = TRUE, out.width = "90%", fig.cap="Source: Medium.com"}
knitr::include_graphics("https://cdn-images-1.medium.com/max/1200/0*F0y1bmOEzCFCcPE_")
```

## {.tabset}

### Overview

By the end of this practical you will know how to:

1. Fit regression, decision trees and random forests to training data
2. Evaluate model fitting *and* prediction performance in a test set
3. Compare the fitting and prediction performance of two models
4. Explore the effects of features on model predictions

### Datasets

```{r, eval = TRUE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
```

|File  |Rows | Columns |
|:----|:-----|:------|
|[college_train.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/college_train.csv)| 500 | 18|
|[college_test.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/college_est.csv)| 277 | 18|
|[house_train.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/house_train.csv)| 5000 | 21|
|[house_test.csv](https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/_data/house_test.csv)| 1000 | 21|


```{r, message = FALSE, eval = TRUE, echo = FALSE}
# Load datasets locally
library(tidyverse)
college_train <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/college_train.csv")
college_test <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/college_test.csv")
house_train <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/house_train.csv")
house_test <- read_csv("https://raw.githubusercontent.com/therbootcamp/appliedML_2019Jan/master/1_Data/house_test.csv")
```

- The `college_train` and `college_test` data are taken from the `College` dataset in the `ISLR` package. To see column descriptions, run the following:

```{r, eval = FALSE}
library(ISLR)   # Load ISLR package
?College        # Look at help menu for College
```

- The `carseats_train` and `carseats_test` data are taken from the `Carseats` dataset in the `ISLR` package. To see column descriptions, run the following:

```{r, eval = FALSE}
library(ISLR)   # Load ISLR package
?Carseats        # Look at help menu for Carseats
```

- The `house_train` and `house_test` data come from [https://www.kaggle.com/harlfoxem/housesalesprediction](https://www.kaggle.com/harlfoxem/housesalesprediction)

### Glossary

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
| `trainControl()`|`caret`|    Define modelling control parameters| 
| `train()`|`caret`|    Train a model|
| `predict(object, newdata)`|`stats`|    Predict the criterion values of `newdata` based on `object`|
| `postResample()`|`caret`|   Calculate aggregate model performance in regression tasks|
| `confusionMatrix()`|`caret`|   Calculate aggregate model performance in classification tasks| 


### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|
|`caret`|`install.packages("caret")`|
|`partykit`|`install.packages("partykit")`|
|`party`|`install.packages("party")`|

### Cheatsheet

<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf">
  <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="Trulli" style="width:70%">
  <figcaption>hhttps://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption></a>
</figure>

### Examples

```{r, eval = FALSE, echo = TRUE}
# Fitting and evaluating regression, decision trees, and random forests

# Step 0: Load packages-----------

library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 
library(partykit)     # For decision trees
library(party)        # For decision trees

# Step 1: Load and Clean, and Explore Training data ----------------------

# training data
data_train <- read_csv("1_Data/mpg_train.csv")

# test data
data_test <- read_csv("1_Data/mpg_test.csv")

# Convert all characters to factor
#  Some ML models require factors

data_train <- data_train %>%
  mutate_if(is.character, factor)

data_test <- data_test %>%
  mutate_if(is.character, factor)

# Explore training data

data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Define criterion_train
#   We'll use this later to evaluate model accuracy

criterion_train <- data_train$hwy

# Step 2: Define training control parameters -------------

# In this case, I will set method = "none" to fit to 
#  the entire dataset without any fancy methods

ctrl_none <- trainControl(method = "none") 

# Step 3: Train model: -----------------------------
#   Criterion: hwy
#   Features: year, cyl, displ

# Regression --------------------------

hwy_glm <- train(form = hwy ~ year + cyl + displ,
                 data = data_train,
                 method = "glm",
                 trControl = ctrl_none)

# Look at summary information
hwy_glm$finalModel
summary(hwy_glm)

# Save fitted values
glm_fit <- predict(hwy_glm)

#  Calculate fitting accuracies
postResample(pred = glm_fit, 
             obs = criterion_train)

# Decision Trees ----------------

hwy_dt <- train(form = hwy ~ year + cyl + displ,
                data = data_train,
                method = "rpart",
                trControl = ctrl_none,
                tuneGrid = expand.grid(cp = .01))   # Set complexity parameter

# Look at summary information
hwy_dt$finalModel
plot(as.party(hwy_dt$finalModel))   # Visualise your trees

# Save fitted values
dt_fit <- predict(hwy_dt)

# Calculate fitting accuracies
postResample(pred = dt_fit, obs = criterion_train)

# Random Forests -------------------------

hwy_rf <- train(form = hwy ~ year + cyl + displ,
                data = data_train,
                method = "rf",
                trControl = ctrl_none,
                tuneGrid = expand.grid(mtry = 2))   # Set number of features randomly selected

# Look at summary information
hwy_rf$finalModel

# Save fitted values
rf_fit <- predict(hwy_rf)

# Calculate fitting accuracies
postResample(pred = rf_fit, obs = criterion_train)

# Visualise Accuracy -------------------------

# Tidy competition results
accuracy <- tibble(criterion_train = criterion_train,
                   Regression = glm_fit,
                   DecisionTrees = dt_fit,
                   RandomForest = rf_fit) %>%
               gather(model, prediction, -criterion_train) %>%
  
  # Add error measures
  mutate(se = prediction - criterion_train,
         ae = abs(prediction - criterion_train))

# Calculate summaries
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of truth versus predictions

ggplot(data = accuracy,
       aes(x = criterion_train, y = prediction, col = model)) +
  geom_point(alpha = .5) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Predicting mpg$hwy",
       subtitle = "Black line indicates perfect performance")

# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Fitting Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))

# Step 5: Access prediction ------------------------------

# Define criterion_train
criterion_test <- data_test$hwy

# Save predicted values
glm_pred <- predict(hwy_glm, newdata = data_test)
dt_pred <- predict(hwy_dt, newdata = data_test)
rf_pred <- predict(hwy_rf, newdata = data_test)

#  Calculate fitting accuracies
postResample(pred = glm_pred, obs = criterion_test)
postResample(pred = dt_pred, obs = criterion_test)
postResample(pred = rf_pred, obs = criterion_test)

# Visualise Accuracy -------------------------

# Tidy competition results
accuracy <- tibble(criterion_test = criterion_test,
                   Regression = glm_pred,
                   DecisionTrees = dt_pred,
                   RandomForest = rf_pred) %>%
               gather(model, prediction, -criterion_test) %>%
  
  # Add error measures
  mutate(se = prediction - criterion_test,
         ae = abs(prediction - criterion_test))

# Calculate summaries
accuracy_agg <- accuracy %>%
                  group_by(model) %>%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of truth versus predictions

ggplot(data = accuracy,
       aes(x = criterion_test, y = prediction, col = model)) +
  geom_point(alpha = .5) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Predicting mpg$hwy",
       subtitle = "Black line indicates perfect performance")

# Plot B) Violin plot of absolute errors

ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = "Prediction Absolute Errors",
       subtitle = "Numbers indicate means",
       x = "Model",
       y = "Absolute Error") +
  guides(fill = FALSE) +
  annotate(geom = "label", 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))
```

# College Graduation Rates

In this section, we will again predict college graduation rates `Grad_Rate` from the `college_train` and `college_test` datasets. Here's how the first few rows of `college_train` should look:

```{r, results = 'asis', echo = FALSE, eval = TRUE}
knitr::kable(college_train[1:10,])
```

## A - Setup

1. Open your `BaselRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data file(s) listed in the `Datasets` section above are in your `1_Data` folder

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `Prediction_College_practical.R` in the `2_Code` folder.  
3. Using `library()` load the set of packages for this practical listed in the packages section above.

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Prediction Practical - With regression, decision trees, and random forests

library(tidyverse)
library(caret)
library(party)
library(partykit)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(tidyverse)
```

4. Run the code below to load each of the datasets listed in the `Datasets` as new objects

```{r, echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE}
# College data
college_train <- read_csv(file = "1_Data/college_train.csv")
college_test <- read_csv(file = "1_Data/college_test.csv")
```

5. Take a look at the first few rows of each dataframe by printing them to the console.

```{r, echo = TRUE, eval = FALSE}
# Print dataframes to the console

college_train
college_test
```

6. Print the numbers of rows and columns of each dataset using the `dim()` function

```{r, echo = TRUE, eval = FALSE}
# Print numbers of rows and columns

dim(XXX)
dim(XXX)
```

7. Look at the names of the dataframes with the `names()` function.

```{r, echo = TRUE, eval = FALSE}
# Print the names of each dataframe

names(XXX)
names(XXX)
```

8. Open each dataset in a new window using `View()`. Do they look ok?

```{r}
# Open each dataset in a window.

View(XXX)
View(XXX)
```

9. We need to do a little bit of data cleaning before starting. Specifically, we need to convert all character columns to factors: Do this by running the following code:

```{r}
# Convert all character columns to factor

college_train <- college_train %>%
          mutate_if(is.character, factor)

college_test <- college_test %>%
          mutate_if(is.character, factor)
```

## B - Fitting

Your goal in this task is to fit models predicting `Grad.Rate`, the percentage of attendees who graduate from each college

1. Using `trainControl()`, set your training control method to `"none"`. Save your object as `ctrl_none`.

```{r, echo = TRUE, eval = FALSE}
# Set training method to "none" for simple fitting
#  Note: This is for demonstration purposes, you would almost
#   never do this for a 'real' prediction task!

ctrl_none <- trainControl(method = "XXX")
```


### Regression

2. Using `train()` fit a regression model called `grad_glm` predicting `Grad.Rate` as a function of all features

- For the `form` argument, use `Grad.Rate ~ .`
- For the `data` argument, use  `college_train` in the data argument
- For the `method` argument, use `method = "glm"` for regression
- For the `trControl` argument, use your `ctrl_none` object you created before

```{r, echo = TRUE, eval = FALSE}
grad_glm <- train(form = XX ~ .,
                  data = XX,
                  method = "XXX",
                  trControl = ctrl_none)
```

3. Explore your `grad_glm` object by looking at `grad_glm$finalModel` and using `summary()`, what do you find?

```{r, eval = FALSE, echo = TRUE}
grad_glm$XXX
summary(XXX)
```

4. Using `predict()` save the fitted values of `grad_glm` object as `glm_fit`:

```{r, echo = TRUE, eval = FALSE}
# Save fitted values of regression model
glm_fit <- predict(XXX)
```

5. Print your `glm_fit` object, look at summary statistics with `summary(glm_fit)`, and create a histogram with `hist()` do they make sense?

```{r}
# Explore regression model fits

XXX
summary(XXX)
hist(XXX)
```

### Decision Trees

4. Using `train()`, fit a decision tree model called `grad_dt`

- For the `form` argument, use `Grad.Rate ~ .`
- For the `data` argument, use  `college_train` in the data argument
- For the `method` argument, use `method = "rpart"` to create decision trees
- For the `trControl` argument, use your `ctrl_none` object you created before
- For the `tuneGrid` argument, use `cp = 0.01` to specify the value of the complexity parameter. This is a pretty low value which means your trees will be, relatively, complex.

```{r}
grad_dt <- train(form = XX ~ .,
                  data = XXX,
                  method = "XX",
                  trControl = XX,
                  tuneGrid = expand.grid(cp = XX))   # Set complexity parameter
```

5. Explore your `grad_dt` object by looking at `grad_dt$finalModel` and plotting it with `plot(as.party(grad_dt$finalModel))`, what do you find?

6. Using `predict()` save the fitted values of `grad_dt` object as `dt_fit`:

```{r, echo = TRUE, eval = FALSE}
# Save fitted values of decision tree model
dt_fit <- predict(XXX)
```

7. Print your `dt_fit` object, look at summary statistics with `summary(dt_fit)`, and create a histogram with `hist()` do they make sense?

```{r}
# Explore decision tree fits

XXX
summary(XXX)
hist(XXX)
```

### Random Forests

8. Using `train()`, fit a random forest model called `grad_rf`

- For the `form` argument, use `Grad.Rate ~ .`
- For the `data` argument, use  `college_train` in the data argument
- For the `method` argument, use `method = "rf"` to fit random forests
- For the `trControl` argument, use your `ctrl_none` object you created before
- For the `mtry` parameter, use `mtry` = 2. This is a relatively low value, so the forest will not be very diverse

```{r, eval = FALSE, echo = TRUE}
grad_rf <- train(form = XX ~ .,   # Predict grad and exclude college_name
                 data = XX,
                 method = "XX",
                 trControl = XX,
                 tuneGrid = expand.grid(mtry = XX))  # Set number of features randomly selected
```

9. Using `predict()` save the fitted values of `grad_rf` object as `rf_fit`:

```{r, echo = TRUE, eval = FALSE}
# Save fitted values of random forest model
rf_fit <- predict(XXX)
```

10. Print your `rf_fit` object, look at summary statistics with `summary(rf_fit)`, and create a histogram with `hist()` do they make sense?

```{r}
# Explore random forest fits

XXX
summary(XXX)
hist(XXX)
```

### Assess accuracy

11. Save the true training criterion values (`college_train$Grad.Rate`) as a vector called `criterion_train`

```{r, echo = TRUE, eval = FALSE}
# Save training criterion values
criterion_train <- XXX$XXX
```

12. Using `postResample()`, determine the fitting performance of each of your models separately. Make sure to set your `criterion_train` values to the `obs` argument, and your true model fits `XX_fit` to the `pred` argument:

```{r, echo = TRUE, eval = FALSE}
# Calculate fitting accuracies of each model
# pred = XX_pred
# obs = criterion_train

# Regression
postResample(pred = XXX, obs = XXX)

# Decision Trees
postResample(pred = XXX, obs = XXX)

# Random Forests
postResample(pred = XXX, obs = XXX)
```

13. Which one had the best fits? What was the fitting MAE of each model?

(Optional). If you'd like to, try visualizing the fitting results using the plotting code shown in the Examples tab above. Ask for help if you need it!

## C - Prediction

1. Save the criterion values from the test data set `college_test$Grad.Rate` as a new vector called `criterion_test`:

```{r}
# Save criterion values
criterion_test <- XXX$XXX
```

2. Using `predict()`, save the predicted values of each model for the test data `college_test` as `glm_pred`, `dt_pred` and `rf_pred`. 

```{r, echo = TRUE, eval = FALSE}
# Save model predictions for test data
# newdata = college_test

# Regression
glm_pred <- predict(XXX, newdata = XXX)

# Decision Trees
dt_pred <- predict(XXX, newdata = XXX)

# Random Forests
rf_pred <- predict(XXX, newdata = XXX)
```

3. Using `postResample()`, determine the *prediction* performance of each of your models against the test criterion `criterion_test`. 

```{r, echo = TRUE, eval = FALSE}
# Calculate prediction accuracies of each model
# obs = criterion_test
# pred = XX_pred

# Regression
postResample(pred = XXX, obs = XXX)

# Decision Trees
postResample(pred = XXX, obs = XXX)

# Random Forests
postResample(pred = XXX, obs = XXX)
```

4. How does each model's *prediction / test* performance (on the `XXX_test` data) compare to its *fitting / training* performance (on the `XXX_train` data)? Is it worse? Better? The same? What does the change tell you about the models?

5. Which of the three models has the best prediction performance?

6. If you had to use one of these three models in the real-world, which one would it be? Why?

7. If someone came to you and asked "If I use your model in the future to predict the graduation rate of a new college, how accurate do you think it would be?", what would you say?

# House Prices in King County, Washington

In this section, we will predict the prices of houses in King County Washington (home of Seattle, [which you can thank for this](https://qz.com/208457/a-cartographic-guide-to-starbucks-global-domination/)) using the `house_train` and `house_test` datasets. Here's how they should look:

```{r, results = 'asis', echo = FALSE, eval = TRUE}
knitr::kable(house_train[1:10,])
```

## A - Setup

1. Make sure you are still working in your `BaselRBootcamp` R project, with the folders `1_Data` and `2_Code`. Make sure that the data file(s) listed in the `Datasets` section above are in your `1_Data` folder

2. Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called `Prediction_HousePrices_practical.R` in the `2_Code` folder.  
3. Using `library()` load the set of packages for this practical listed in the packages section above.

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Fitting Practical - House Prices Prediction

library(tidyverse)
library(caret)
library(party)
library(partykit)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(tidyverse)
```

4. Run the code below to load each of the datasets listed in the `Datasets` as new objects

```{r, echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE}
# house data
house_train <- read_csv(file = "1_Data/house_train.csv")
house_test <- read_csv(file = "1_Data/house_test.csv")
```

5. Take a look at the first few rows of each dataframe by printing then to the console.

```{r, echo = FALSE, eval = FALSE}
# Print dataframes to the console

house_train
house_test
```

6. Print the numbers of rows and columns of each dataset using the `dim()` function

```{r, echo = FALSE, eval = FALSE}
# Print numbers of rows and columns

dim(XXX)
dim(XXX)
```

7. Look at the names of the dataframes with the `names()` function.

```{r, echo = FALSE, eval = FALSE}
# Print the names of each dataframe

names(XXX)
names(XXX)
```

8. Open each dataset in a new window using `View()`. Do they look ok?

```{r, echo = FALSE, eval = FALSE}
# Open each dataset in a window.

View(XXX)
View(XXX)
```

9. We need to do a little bit of data cleaning before starting. Specifically, we need to convert all character columsn to factor: Do this by running the following code:

```{r, echo = TRUE, eval = FALSE}
# Convert all character columns to factor

house_train <- house_train %>%
          mutate_if(is.character, factor)

house_test <- house_test %>%
          mutate_if(is.character, factor)
```

## B - Fitting

Your goal in the following models is to predict `price`, the selling price of homes in King County WA.

1. Using `trainControl()`, set your training control method to `"none"`. Save your object as `ctrl_none`.

```{r, echo = FALSE, eval = FALSE}
# Set training method to "none" for simple fitting
#  Note: This is for demonstration purposes, you would almost
#   never do this for a 'real' prediction task!

ctrl_none <- trainControl(method = "XXX")
```

### Regression

2. Using `train()` fit a regression model called `price_glm` predicting `price` as a function of all features *except* for id and date (Ask yourself, why do you think we are excluding this?).

- For the `form` argument, use `price ~ . - id - date`
- For the `data` argument, use  `house_train` in the data argument
- For the `method` argument, use `method = "glm"` for regression
- For the `trControl` argument, use your `ctrl_none` object you created before

```{r, echo = TRUE, eval = FALSE}
price_glm <- train(form = XX ~ . - XX - XX,
                  data = XX,
                  method = "XXX",
                  trControl = ctrl_none)
```

3. Explore your `price_glm` object by looking at `price_glm$finalModel` and using `summary()`, what do you find?

```{r, eval = FALSE, echo = TRUE}
price_glm$XXX
summary(XXX)
```

4. Using `predict()` save the fitted values of `price_glm` object as `glm_fit`:

```{r, echo = FALSE, eval = FALSE}
# Save fitted values of regression model
glm_fit <- predict(XXX)
```

5. Print your `glm_fit` object, look at summary statistics with `summary(glm_fit)`, and create a histogram with `hist()` do they make sense?

```{r, echo = FALSE, eval = FALSE}
# Explore regression model fits

XXX
summary(XXX)
hist(XXX)
```

### Decision Trees

4. Using `train()`, fit a decision tree model called `price_dt` predicting `price` (using the same features as above).

- For the `form` argument, use `price ~ . -id - date`
- For the `data` argument, use  `house_train` in the data argument
- For the `method` argument, use `method = "rpart"` to create decision trees
- For the `trControl` argument, use your `ctrl_none` object you created before
- For the `tuneGrid` argument, use `cp = 0.01` to specify the value of the complexity parameter. This is a pretty low value which means your trees will be, relatively, complex.

```{r, echo = FALSE, eval = FALSE}
price_dt <- train(form = XX ~ . - XX - XX,
                  data = XXX,
                  method = "XX",
                  trControl = XX,
                  tuneGrid = expand.grid(cp = XX))   # Set complexity parameter
```

5. Explore your `price_dt` object by looking at `price_dt$finalModel` and plotting it with `plot(as.party(price_dt$finalModel))`, what do you find?

6. Using `predict()` save the fitted values of `price_dt` object as `dt_fit`:

```{r, echo = FALSE, eval = FALSE}
# Save fitted values of decision tree model
dt_fit <- predict(XXX)
```

7. Print your `dt_fit` object, look at summary statistics with `summary(dt_fit)`, and create a histogram with `hist()` do they make sense?

```{r, echo = FALSE, eval = FALSE}
# Explore decision tree fits

XXX
summary(XXX)
hist(XXX)
```

### Random Forests

8. Using `train()`, fit a random forest model called `price_rf` predicting `price` (using the same features as above).

- For the `form` argument, use `price ~ . - id - date`
- For the `data` argument, use  `house_train` in the data argument
- For the `method` argument, use `method = "rf"` to fit random forests
- For the `trControl` argument, use your `ctrl_none` object you created before
- For the `mtry` parameter, use `mtry` = 2. This is a relatively low value, so the forest will not be very diverse

```{r, echo = FALSE, eval = FALSE}
price_rf <- train(form = XX ~ . - XX - XX,   # Predict price and exclude id and date
                 data = XX,
                 method = "XX",
                 trControl = XX,
                 tuneGrid = expand.grid(mtry = XX))  # Set number of features randomly selected
```

9. Using `predict()` save the fitted values of `price_rf` object as `rf_fit`:

```{r, echo = FALSE, eval = FALSE}
# Save fitted values of random forest model
rf_fit <- predict(XXX)
```

10. Print your `rf_fit` object, look at summary statistics with `summary(rf_fit)`, and create a histogram with `hist()` do they make sense?

```{r, echo = FALSE, eval = FALSE}
# Explore random forest fits

XXX
summary(XXX)
hist(XXX)
```

### Assess accuracy

11. Save the true training criterion values (`house_train$price`) as a vector called `criterion_train`

```{r, echo = FALSE, eval = FALSE}
# Save training criterion values
criterion_train <- XXX$XXX
```

12. Using `postResample()`, determine the fitting performance of each of your models separately. Make sure to set your `criterion_train` values to the `obs` argument, and your true model fits `XX_fit` to the `pred` argument:

```{r, echo = FALSE, eval = FALSE}
# Calculate fitting accuracies of each model
# pred = XX_fit
# obs = criterion_train

# Regression
postResample(pred = XXX, obs = XXX)

# Decision Trees
postResample(pred = XXX, obs = XXX)

# Random Forests
postResample(pred = XXX, obs = XXX)
```

13. Which one had the best fits? What was the fitting MAE of each model?

(Optional). If you'd like to, try visualizing the fitting results using the plotting code shown in the Examples tab above. Ask for help if you need it!

## C - Prediction

1. Save the criterion values from the test data set `house_test$price` as a new vector called `criterion_test`:

```{r, echo = FALSE, eval = FALSE}
# Save criterion values
criterion_test <- XXX$XXX
```

2. Using `predict()`, save the predicted values of each model for the test data `house_test` as `glm_pred`, `dt_pred` and `rf_pred`. 

```{r, echo = FALSE, eval = FALSE}
# Save model predictions for test data
# XX_mod
# newdata = college_test

# Regression
glm_pred <- predict(XXX, newdata = XXX)

# Decision Trees
dt_pred <- predict(XXX, newdata = XXX)

# Random Forests
rf_pred <- predict(XXX, newdata = XXX)
```

3. Using `postResample()`, determine the *prediction* performance of each of your models against the test criterion `criterion_test`. 

```{r, echo = FALSE, eval = FALSE}
# Calculate prediction accuracies of each model
# obs = criterion_test
# pred = XX_pred

# Regression
postResample(pred = XXX, obs = XXX)

# Decision Trees
postResample(pred = XXX, obs = XXX)

# Random Forests
postResample(pred = XXX, obs = XXX)
```

4. How does each model's *prediction / test* performance (on the `XXX_test` data) compare to its *fitting / training* performance (on the `XXX_train` data)? Is it worse? Better? The same? What does the change tell you about the models?

5. Which of the three models has the best prediction performance?

6. If you had to use one of these three models in the real-world, which one would it be? Why?

7. If someone came to you and asked "If I use your model in the future to predict the graduation rate of a new college, how accurate do you think it would be?", what would you say?

# D - Exploring model tuning parameters

1. In all of your decision tree models so far, you have been setting the complexity parameter to 0.01. Try setting it to a larger value of 0.2 and see how your decision trees change (by plotting them). Do they get more or less complicated? How does increasing this value affect fitting and prediction performance? If you are interested in learning more about this parameter, look at the help menu with `?rpart.control`.

2. In each of your random forest models, you have been setting the `mtry` argument to 2. Try setting it to a larger value such as 5 and re-running your models. How does increasing this value affect fitting and prediction performance? If you are interested in learning more about this parameter, look at the help menu with `?randomForest`.

3. By default, the `train()` function uses 500 trees in `method = "rf"`. How do the number of trees affect performance? To answer this, try setting the number of trees to 1,000 (see example below) and re-evaluating your model's training and test performance. What do you find? What if you set the number of trees to just 10?

```{r, eval = FALSE}
# Create random forest model with 1000 trees

mod <- train(form = hwy ~ year + cyl + displ,
             data = data_train,
             method = "rf",
             trControl = train_control,
             ntree = 1000,   # use 1000 trees! (Instead of the default value of 500)
             tuneGrid = expand.grid(mtry = 2))

```

# Z - Challenges

1. So far you've probably been using most, if not all, available features in predicting house sales. But imagine someone came to you and said "I need to know how much a set of new houses will sell for, but I only have access to three features `bedrooms`, `bathrooms`, and `sqft_living`. Which of your models should I use and how accurate will they be? How would you answer that question? Use your modelling techniques to find out!

2. Repeat your modelling process, but now do a classification task. Specifically, predict whether or not a house sells for at least \$1,000,000. To do this, you'll first need to create a new column called `million` in both your `house_train` and `house_test` datasets (the code below should help you). Then, use your best modelling techniques to make this prediction. How accurate are your models in predicting whether or not a house will sell for over $1,000,000? Don't forget to use the `confusionMatrix()` function instead of `postResample()` to evaluate your model's accuracy!

```{r}
# Add million column to house_train and house_test
#  A factor indicating whether or not a house sells for
#   over 1,000,0000

house_train <- house_train %>%
  mutate(million = factor(price > 1000000))

house_test <- house_test %>%
  mutate(million = factor(price > 1000000))
```
