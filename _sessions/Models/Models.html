<!DOCTYPE html>
<html>
  <head>
    <title>Models</title>
    <meta charset="utf-8">
    <meta name="author" content="Applied Machine Learning with R www.therbootcamp.com @therbootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Models
### Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### January 2019

---


layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;Applied Machine Learning with R, January 2019&lt;/font&gt;&lt;/a&gt;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;www.therbootcamp.com&lt;/font&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/div&gt; 

---









# There is no free lunch

.pull-left35[

&lt;u&gt;Theorem&lt;/u&gt;

Given a finite set `\(V\)` and a finite set `\(S\)` of real numbers, &lt;high&gt;assume that `\(f:V\to S\)` is chosen at random&lt;/high&gt; according to uniform distribution on the set `\(S^{V}\)` of all possible functions from `\(V\)` to `\(S\)`. For the problem of optimizing `\(f\)` over the set `\(V\)`, &lt;high&gt;then no algorithm performs better than blind search.&lt;/high&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;a href="https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf"&gt;Wolpert &amp; Macready, 1997, No Free Lunch Theorems for Optimization&lt;/a&gt;

]

.pull-right55[

&lt;p align="left"&gt;
  &lt;br&gt;
  &lt;img src="image/free_lunch.jpg" height=400px width=650px&gt;
&lt;/p&gt;

]

---

.pull-left4[

# Know your problem

&lt;u&gt;Bias-variance trade-off&lt;/u&gt;

&lt;font size=5&gt;&lt;high&gt;Error&lt;/high&gt; = &lt;high&gt;Bias&lt;/high&gt; + &lt;high&gt;Variance&lt;/high&gt;&lt;/font&gt;

&lt;br&gt;
Simply put...

&lt;high&gt;Bias&lt;/high&gt; arises from strong &lt;high&gt;model assumptions&lt;/high&gt; not being met by the environment.

&lt;high&gt;Bias&lt;/high&gt; arises from high &lt;high&gt;model flexibility&lt;/high&gt; fitting the noise in the data (i.e., overfitting).

&lt;br&gt;&lt;br&gt;
&amp;#8594; &lt;high&gt;Make strong assumptions&lt;/high&gt; (use simple models), if possible.

]

.pull-right45[

&lt;p align="left"&gt;
  &lt;br&gt;
  &lt;img src="image/bias_variance.png" height=580px&gt;
&lt;/p&gt;

]


---

.pull-left4[
# Linear or non-linear
&lt;br&gt;

One important model assumptions concerns linearity.
&lt;br&gt;&lt;br&gt;
&lt;high&gt;Linear models&lt;/high&gt; (`lm`, `glm`) make strong model assumptions. They are more often wrong, but also ceteris paribus &lt;high&gt;less prone to overfitting&lt;/high&gt;.

&lt;high&gt;Non-linear moels&lt;/high&gt; (everything else) make weaker model assumptions, leaving the exact relationship (more) open. They are are closer to the truth, but also ceteris paribus &lt;high&gt;more prone to overfitting&lt;/high&gt;. 


]

.pull-right5[

&lt;p align="center"&gt;
  &lt;br&gt;&lt;br&gt;&lt;br&gt;
  &lt;img src="image/linearity.png" height=480px&gt;
&lt;/p&gt;

]

---

.pull-left45[

# Kernel trick

&lt;high&gt;Transforms "input space" into new "feature space"&lt;/high&gt; to allows for object separation.

&lt;p align="center"&gt;
  &lt;img src="image/kernel_bw.png" height=160px&gt;
&lt;/p&gt;

Used in &lt;high&gt;Support Vector Machines&lt;/high&gt; (e.g., `method = "svmRadial"`) often using a &lt;high&gt;radial basis function&lt;/high&gt; (rdf).

&lt;p align="center"&gt;
  &lt;img src="image/rdf_kernel.png" width=300px&gt;
&lt;/p&gt;

Kernels &lt;high&gt;re-represent objects&lt;/high&gt; in terms of other objects!

]


.pull-right5[

&lt;p align="center"&gt;
  &lt;br&gt;&lt;br&gt;&lt;br&gt;
  &lt;img src="image/linearity.png" height=480px&gt;
&lt;/p&gt;

]

---

# Automatic feature engineering

&lt;high&gt;Deep learning&lt;/high&gt; aka neural networks and, espectially, &lt;high&gt;convolutional neural networks&lt;/high&gt;, excel because they generate their features. 

Neural networks are not the focus of `caret` and this course. Powerful implementations based on &lt;high&gt;Google's Tensorflow&lt;/high&gt; libarary are provided by `tensorflow`.


.pull-left3[

&lt;p align="center"&gt;
  &lt;br&gt;
  &lt;img src="image/tf.png"&gt;
&lt;/p&gt;


]

.pull-right65[

&lt;p align="center"&gt;
  &lt;img src="image/power_of_deeplearning.png" height=300px&gt;
&lt;/p&gt;


]


---

# Robustness

.pull-left4[

To produce &lt;high&gt;robust predictions&lt;/high&gt; that &lt;high&gt; suffer less from variance&lt;/high&gt; ML models use a variety of &lt;high&gt;tricks&lt;/high&gt;.

&lt;p align="center"&gt;
  &lt;br&gt;&lt;br&gt;
  &lt;img src="image/robustness_sel.png" width=350px&gt;
&lt;/p&gt;

]



.pull-right55[
&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="210"&gt;
  &lt;col width="210"&gt;
  &lt;col width="210"&gt;
&lt;tr&gt;
  &lt;th&gt;Approach&lt;/th&gt;
  &lt;th&gt;Implementation&lt;/th&gt;
  &lt;th&gt;Examples&lt;/th&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Tolerance&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;Decrease error tolerance&lt;/td&gt;
  &lt;td align="center"&gt;&lt;mono&gt;svmRadial&lt;/mono&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Regularization&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;Penalize for complexity&lt;/td&gt;
  &lt;td align="center"&gt;&lt;mono&gt;lasso&lt;/mono&gt;, &lt;mono&gt;ridge&lt;/mono&gt;, &lt;mono&gt;elasticnet&lt;/mono&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Ensemble&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;Bagging&lt;/td&gt;
  &lt;td align="center"&gt;&lt;mono&gt;treebag&lt;/mono&gt;, &lt;mono&gt;randomGLM&lt;/mono&gt;, &lt;mono&gt;randomForest&lt;/mono&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Ensemble&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;Boosting&lt;/td&gt;
  &lt;td align="center"&gt;&lt;mono&gt;adaboost&lt;/mono&gt;&lt;mono&gt;xgbTree&lt;/mono&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Feature selection&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;Regularization&lt;/td&gt;
  &lt;td align="center"&gt;&lt;mono&gt;lasso&lt;/mono&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Feature selection&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;Importance&lt;/td&gt;
  &lt;td align="center"&gt;&lt;mono&gt;random forest&lt;/mono&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

]



---

# Regularization

.pull-left45[

Regularization is the process of adding model terms, usually &lt;high&gt;penalties for complexity&lt;/high&gt;, in order to prevent overfitting (or solve a problem in the first place).

&lt;br2&gt;
&lt;p align = 'center'&gt;&lt;font size=5&gt;&lt;high&gt;Loss&lt;/high&gt; = &lt;high&gt;Misfit&lt;/high&gt; + &lt;high&gt;Penalty&lt;/high&gt;&lt;/font&gt;&lt;/p&gt;
&lt;br&gt;

&lt;table style="cellspacing:0; cellpadding:0; border:none;"&gt;
  &lt;col width="160"&gt;
  &lt;col width="160"&gt;
  &lt;col width="160"&gt;
&lt;tr&gt;
  &lt;th&gt;Name&lt;/th&gt;
  &lt;th&gt;Penalty&lt;/th&gt;
  &lt;th&gt;`caret`&lt;/th&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;AIC/BIC&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;&lt;img src="image/regularization/aicbic.png" height=24px&gt;&lt;/td&gt;
  &lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Lasso&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;&lt;img src="image/regularization/lasso.png" height=24px&gt;&lt;/td&gt;
  &lt;td align="center"&gt;`method = "lasso"`&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Ridge&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;&lt;img src="image/regularization/ridge.png" height=24px&gt;&lt;/td&gt;
  &lt;td align="center"&gt;`method = "ridge"`&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="background-color:#ffffff"&gt;
  &lt;td align="center"&gt;&lt;high&gt;Elastic Net&lt;/high&gt;&lt;/td&gt;
  &lt;td align="center"&gt;&lt;img src="image/regularization/ridge.png" height=24px&gt;&lt;/td&gt;
  &lt;td align="center"&gt;`method = "elasticnet"`&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

]



.pull-right5[

&lt;img src="Models_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

]

---

.pull-left45[

# Bagging

&lt;high&gt;Aggregate predictions from multiple fits to resampled data.&lt;/high&gt;

Especially beneficial for models that produce relatively instable solutions, e.g., regression trees. `rpart` &amp;#8594; `treebag`.

Random forest adds sampling of features to reduce dependencies across trees.

&lt;br&gt;
&lt;u&gt;Algorithm&lt;/u&gt;
1. &lt;high&gt;Resample&lt;/high&gt; data (without replacement)
2. &lt;high&gt;Fit&lt;/high&gt; model to resampled data
3. &lt;high&gt;Average&lt;/high&gt; predictions

]

.pull-right45[

&lt;p align="center"&gt;
  &lt;br&gt;&lt;br&gt;&lt;br&gt;
  &lt;img src="image/münchhausen.jpg" height=450px&gt;
&lt;/p&gt;

]

---

# Boosting

.pull-left4[

Iterative algorithm that adaptively increases the weight given to previously misclassified samples.

New versions of the classic `adaboost` algorithm, e.g., `xgbTree`, &lt;high&gt;belong to the best ML models out there&lt;/high&gt;. 

&lt;u&gt;Algorithm&lt;/u&gt;
1. Assign &lt;high&gt;equal weight&lt;/high&gt; to samples
2. &lt;high&gt;Fit&lt;/high&gt; simple model
3. &lt;high&gt;Increase weight of misfit samples&lt;/high&gt; by model misfit for next iteration
5. &lt;high&gt;Average predictions weighted by model misfit&lt;/high&gt; 

]


.pull-right5[

&lt;p align="center"&gt;
  
  &lt;img src="image/bagg_boost.png" height=410px&gt;

&lt;/p&gt;

]

---

# Automatic feature selection

.pull-left45[

Many models reduce complexity by automatically relying on a subset of good features. 

&lt;u&gt;Two examples&lt;/u&gt;

&lt;high&gt;LASSO&lt;/high&gt;

Regularization, in particular via `lasso`, frequently &lt;high&gt;estimates &lt;mono&gt;beta = 0&lt;/mono&gt;&lt;/high&gt; and, thus, essentially deselects that feature. 

&lt;high&gt;Random forests&lt;/high&gt;

As random forests select at any node the best of `mtry`-many randomly selected features, &lt;high&gt;unpredictive features may never come to action&lt;/high&gt;. This is especially true for large `mtry`. 


]


.pull-right45[

&lt;p align="center"&gt;
  
  &lt;img src="image/self_tuning.png" height=420px&gt;
  
&lt;/p&gt;

]

---

# Excursus: Unsupervised learning

.pull-left5[

Unsupervised learning aims to &lt;high&gt;identify structure in the absence of labels&lt;/high&gt;, i.e., a criterian. 

There is &lt;high&gt;no ground truth&lt;/high&gt;, rendering unsupervised learning problems essentially &lt;high&gt;impossible to "solve"&lt;/high&gt;, i.e., you never quite know how good a solution is.

&lt;u&gt;Common questions&lt;/u&gt;

Are there &lt;high&gt;groups of cases&lt;/high&gt; (clusters), which case belongs to which group, and how many groups are there? &amp;#8594; `k-means` or `hierarchical clustering`

Are there &lt;high&gt;groups of features&lt;/high&gt;, which features belongs to which group, and how many groups are there? &amp;#8594; `pca` or `svd`

]


.pull-right45[

&lt;p align="center"&gt;
  
  &lt;img src="image/clustering.png" height=400px&gt;
  
&lt;/p&gt;

]



---

# Excurse: Unsupervised learning

.pull-left45[

`k-means`, `hierarchical-clustering`, and other &lt;high&gt;clustering algorithms&lt;/high&gt; attempt to find distributed membership to `\(k\)` groups (clusters) such that &lt;high&gt;groups are maximally homogenous&lt;/high&gt;.

&lt;u&gt;k-means&lt;/u&gt;

Assign cases to the closest centroids (high-dimensional means) while iteratively shifting them around to &lt;high&gt;minimize within-group variance versus between-group variance&lt;/high&gt;. 

&lt;u&gt;´hierarchical clustering`&lt;/u&gt;

Place every case in one group. &lt;high&gt;Join clusters according to a pre-specified distance function&lt;/high&gt; until the desired number of `\(k\)` clusters is reached.  

]

.pull-right5[

&lt;p align="center"&gt;
  
  &lt;img src="image/iris_kmeans.png" height=400px&gt;
  
&lt;/p&gt;

]

---

# Unsupervised learning

pca &amp; svd


.pull-right45[

&lt;p align = "center"&gt;
&lt;img src="image/pca.png"&gt;
&lt;/p&gt;

]

---

# Remember

.pull-left45[

&lt;i&gt;"…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used."&lt;/i&gt;

Pedro Domingos

&lt;br&gt;&lt;br&gt;

&lt;i&gt;"The algorithms we used are very standard for Kagglers. […] We spent most of our efforts in feature engineering. [...] We were also very careful to discard features likely to expose us to the risk of over-fitting our model."&lt;/i&gt;

Xavier Conort

]

.pull-right45[

&lt;p align="center"&gt;
  &lt;img src="image/albert.jpeg" &gt;
&lt;/p&gt;

]





---


# Practical

class: middle, center

&lt;h1&gt;&lt;a href=https://therbootcamp.github.io/appliedML_2019Jan/_sessions/Models/Models_practical.html&gt;Practical&lt;/a&gt;&lt;/h1&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
