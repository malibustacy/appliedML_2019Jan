---
title: "Models"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
date: "January 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer"><span>
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">Applied Machine Learning with R, January 2019</font></a>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">www.therbootcamp.com</font></a>
</span></div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
# Code to knit slides
#baselers <- readr::read_csv("../_data/baselers.csv")
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
```

```{r, echo = FALSE ,message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, 
                      message = FALSE, warning = FALSE, fig.align = 'center', dpi = 200)
library(tidyverse)
library(baselers)
library(ggthemes)
```


# There is no free lunch

.pull-left35[

<u>Theorem</u>

Given a finite set $V$ and a finite set $S$ of real numbers, <high>assume that $f:V\to S$ is chosen at random</high> according to uniform distribution on the set $S^{V}$ of all possible functions from $V$ to $S$. For the problem of optimizing $f$ over the set $V$, <high>then no algorithm performs better than blind search.</high>
<br><br><br><br>
<a href="https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf">Wolpert & Macready, 1997, No Free Lunch Theorems for Optimization</a>

]

.pull-right55[

<p align="left">
  <br>
  <img src="image/free_lunch.jpg" height=400px width=650px>
</p>

]

---

# Know your environment

Bias variance


---

# Non-linearity

complex but sometimes necessary


---

# Kernel trick

complex but sometimes necessary


---

# Robustness

complex but sometimes necessary


---

# Regularization

complex but sometimes necessary



---

# Hedging

complex but sometimes necessary



---

# Binarization

complex but sometimes necessary


---

# Feature selection

complex but sometimes necessary


---

# Feature generation

complex but sometimes necessary


---

# Deep learning

complex but sometimes necessary




---

Supervised learning

Regularization
  automatic feature selection

Kernel trick

Ensembles
  Bagging
  Ensemble
  Boosting

Feature generation
  Random Forest
  Neural networks
  CNN

Unspervised learning

  kernel trick

  k-means
  SVD
  PCA




---


# Practical

<font size=6><b><a href="https://therbootcamp.github.io/Intro2DataScience_2018Oct/_sessions/Plotting/Plotting_practical.html">Link to practical</a>






