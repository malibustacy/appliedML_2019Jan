---
title: "Models"
author: "Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
date: "January 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer"><span>
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">Applied Machine Learning with R, January 2019</font></a>
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
<a href="https://therbootcamp.github.io/"><font color="#7E7E7E">www.therbootcamp.com</font></a>
</span></div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
# Code to knit slides
#baselers <- readr::read_csv("../_data/baselers.csv")
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
```

```{r, echo = FALSE ,message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, 
                      message = FALSE, warning = FALSE, fig.align = 'center', dpi = 200)
library(tidyverse)
library(baselers)
library(ggthemes)
```


# There is no free lunch

.pull-left35[

<u>Theorem</u>

Given a finite set $V$ and a finite set $S$ of real numbers, <high>assume that $f:V\to S$ is chosen at random</high> according to uniform distribution on the set $S^{V}$ of all possible functions from $V$ to $S$. For the problem of optimizing $f$ over the set $V$, <high>then no algorithm performs better than blind search.</high>
<br><br><br><br>
<a href="https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf">Wolpert & Macready, 1997, No Free Lunch Theorems for Optimization</a>

]

.pull-right55[

<p align="left">
  <br>
  <img src="image/free_lunch.jpg" height=400px width=650px>
</p>

]

---

.pull-left4[

# Know your problem

<u>Bias-variance trade-off</u>

<font size=5><high>Error = Bias + Variance</high></font>

<br>
Simply put...

<high>Bias</high> arises from strong <high>model assumptions</high> not being met by the environment.

<high>Bias</high> arises from high <high>model flexibility</high> fitting the noise in the data (i.e., overfitting).

<br><br>
&#8594; <high>Make strong assumptions</high> (use simple models), if possible.

]

.pull-right45[

<p align="left">
  <br>
  <img src="image/bias_variance.png" height=580px>
</p>

]


---

.pull-left4[
# Linear or non-linear
<br>

One important model assumptions concerns linearity.
<br><br>
<high>Linear models</high> (`lm`, `glm`) make strong model assumptions. They are more often wrong, but also ceteris paribus <high>less prone to overfitting</high>.

<high>Non-linear moels</high> (everything else) make weaker model assumptions, leaving the exact relationship (more) open. They are are closer to the truth, but also ceteris paribus <high>more prone to overfitting</high>. 


]

.pull-right55[

<p align="center">
  <br><br><br>
  <img src="image/linearity.png" height=500px>
</p>

]

---

.pull-left4[

# Kernel trick

Transforms "input space" into new "feature space".


<p align="center">
  <img src="image/rdf_kernel.png" height=180px>
</p>



<p align="center">
  <img src="image/kernel_bw.png" height=180px>
</p>

]


.pull-right55[

<p align="center">
  <br><br><br>
  <img src="image/linearity.png" height=500px>
</p>

]


---

# Robustness

complex but sometimes necessary


---

# Regularization

complex but sometimes necessary



---

# Hedging

complex but sometimes necessary



---

# Binarization

complex but sometimes necessary


---

# Feature selection

complex but sometimes necessary


---

# Feature generation

complex but sometimes necessary


---

# Deep learning

complex but sometimes necessary




---

Supervised learning

Regularization
  automatic feature selection

Kernel trick

Ensembles
  Bagging
  Ensemble
  Boosting

Feature generation
  Random Forest
  Neural networks
  CNN

Unspervised learning

  kernel trick

  k-means
  SVD
  PCA




---


# Practical

<font size=6><b><a href="https://therbootcamp.github.io/Intro2DataScience_2018Oct/_sessions/Plotting/Plotting_practical.html">Link to practical</a>






