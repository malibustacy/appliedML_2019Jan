<!DOCTYPE html>
<html>
  <head>
    <title>Optimization</title>
    <meta charset="utf-8">
    <meta name="author" content="Applied Machine Learning with R www.therbootcamp.com @therbootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Optimization
## Regularization and Cross-Validation
### Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### January 2019

---


layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;Applied Machine Learning with R, January 2019&lt;/font&gt;&lt;/a&gt;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;www.therbootcamp.com&lt;/font&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/div&gt; 

---










.pull-left4[
&lt;br&gt;&lt;br&gt;
# Where we are

- &lt;high&gt;Train&lt;/high&gt; one of several models (&lt;high&gt;regression&lt;/high&gt;, &lt;high&gt;decision trees&lt;/high&gt;, and &lt;high&gt;random forests&lt;/high&gt;) on training data.

- Explore models - show regression coefficients, plot decision trees (etc)

- Assess model &lt;high&gt;prediction&lt;/high&gt; performance on &lt;high&gt;test&lt;/high&gt; data
    - Mean Absolute Error (MAE)

]

.pull-right55[

### Model Training

&lt;img src="image/model_training_flow.png" width="1442" style="display: block; margin: auto;" /&gt;

### Model Testing

&lt;img src="image/model_testing_flow.png" width="1439" style="display: block; margin: auto;" /&gt;

]

---


.pull-left4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
# Overfitting (Recap)

When a model is consistently &lt;high&gt;less accurate in predicting future data&lt;/high&gt; than in &lt;high&gt;fitting training data&lt;high&gt;, this is called &lt;high&gt;overfitting&lt;/high&gt;

Just because model A is better than model B in training, does not mean it will be better in testing!

Extremely flexible models that tend to overfit are like 'wolves in sheep's clothing'

]


.pull-right55[


&lt;br&gt;&lt;br&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/wolf_complex.png" alt="&amp;lt;font size = 4&amp;gt;victoriarollison.com (adapted)&amp;lt;/font&amp;gt;" width="100%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;victoriarollison.com (adapted)&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]

---

.pull-left4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;

# Overfitting (Recap)

### How will we try to avoid overfitting?

Use regression models with &lt;high&gt;regularization&lt;/high&gt; terms, such as &lt;high&gt;ridge&lt;/high&gt; and &lt;high&gt;lasso&lt;/high&gt; which explicitly &lt;high&gt;punish model complexity&lt;/high&gt;.

Use &lt;high&gt;cross-validation&lt;/high&gt; to find &lt;high&gt;optimal tuning parameters&lt;/high&gt;, including regularization.

]

.pull-right55[

### Regularised Regression

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/lasso_penalty_eq.jpg" alt="L1 Lasso Penalty" width="100%" /&gt;
&lt;p class="caption"&gt;L1 Lasso Penalty&lt;/p&gt;
&lt;/div&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---

.pull-left55[

# Tuning parameters (Recap)

### What are tuning parameters?

&lt;high&gt;Tuning parameters&lt;/high&gt; are parameters that &lt;high&gt;guide&lt;/high&gt; (aka. 'tune') a model during fitting.

- Decision trees: complexity tuning parameter &lt;high&gt;cp&lt;/high&gt;
- Random forests diversity tuning parameter &lt;high&gt;mtry&lt;/high&gt;
    
Tuning parameters never show up in the final model! They are only used to guide fitting.

There is not one 'best' tuning parameter, it always depends on your specific dataset.

]

.pull-right45[

&lt;img src="image/complexity_parameter.png" width="80%" style="display: block; margin: auto;" /&gt;

&lt;img src="image/mtry_parameter.png" width="80%" style="display: block; margin: auto;" /&gt;

]


---

# Regularised Regression

There are two common methods to fit penalised (aka regularised) regression models: Ridge and Lasso. Each penalises regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left5[

### Ridge

The Lasso penalty is known as the `\(\ell2\)` norm, where Beta weights are selected by minimising the following equation:

&lt;img src="image/ridge_penalty_eq.jpg" width="100%" style="display: block; margin: auto;" /&gt;

As `\(\lambda\)` increases, coefficients are pushed towards (but not necessarily exactly to) 0.

]

.pull-right45[


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/ridge_penalty_plot.png" alt="&amp;lt;font size = 4&amp;gt;James et al., ISLR&amp;lt;/font&amp;gt;" width="80%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;James et al., ISLR&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]




---

# Regularised Regression

There are two common methods to fit penalised (aka regularised) regression models: Ridge and Lasso. Each penalises regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left3[

### Ridge

To fit Ridge penalised regression in R, use `method = "glmnet`.

In the `tuneGrid` argument: 

- `alpha = 0` indicates the `\(\ell2\)` Ridge penalty.
- `lambda` = Vector of lambda tuning parameters values to try.

]

.pull-right65[
&lt;br&gt;

```r
# Train ridge penalised regression model in R

train(form = criterion ~ .,
      data = data_train,
      method = "glmnet",
      trControl = ctrl,
      tuneGrid = expand.grid(alpha = 0, # Ridge penalty
                              lambda = 1:100)) # Lambda
```

]

---

# Regularised Regression

There are two common methods to fit penalised (aka regularised) regression models: Ridge and Lasso. Each penalises regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left5[

### Lasso

The Lasso penalty is known as the `\(\ell1\)` norm, where Beta weights are selected by minimising the following equation:

&lt;img src="image/lasso_penalty_eq.jpg" width="100%" style="display: block; margin: auto;" /&gt;

As `\(\lambda\)` increases, coefficients are pushed towards 0, with some being forced to &lt;high&gt;exactly 0&lt;/high&gt;.

]

.pull-right45[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/lasso_penalty_plot.png" alt="&amp;lt;font size = 4&amp;gt;James et al., ISLR&amp;lt;/font&amp;gt;" width="80%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;James et al., ISLR&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]

---

# Regularised Regression

There are two common methods to fit penalised (aka regularised) regression models: Ridge and Lasso. Each penalises regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left3[

### Lasso

To fit Lasso penalised regression in R, use `method = "glmnet`.

In the `tuneGrid` argument: 

- `alpha = 1` indicates the `\(\ell1\)` Lasso penalty
- `lambda` = Vector of lambda tuning parameters values to try.

]

.pull-right65[
&lt;br&gt;

```r
# Train Lasso penalised regression model in R

train(form = criterion ~ .,
      data = data_train,
      method = "glmnet",
      trControl = ctrl,
      tuneGrid = expand.grid(alpha = 1, # Lasso penalty
                              lambda = 1:100)) # Lambda
```

]


---

.pull-left45[

# K-Fold Cross-Validation

### What is it?

Cross-validation is a sampling procedure performed on training data used to &lt;high&gt;estimate a model's prediction performance&lt;/high&gt; in future test data, and to determine &lt;high&gt;optimal tuning parameters&lt;/high&gt; selected to minimize prediction error.

Cross-validation is not "cheating: because it is only performed on the training data (never on the true test dataset)

After cross-validation is complete, the model is trained on the entire dataset, using optimal tuning parameters, resulting in a &lt;high&gt;final model&lt;/high&gt; which can be used for future model testing.

]

.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="888" style="display: block; margin: auto;" /&gt;

]

---

.pull-left45[

# K-Fold Cross-Validation

### Steps

1) Split the original training data into K 'folds' (mutually exclusive groups of cases)

2) Select K - 1 folds for training, and 1 fold for testing.

3) Fit the model to the K - 1 training folds, and evaluate its testing accuracy on the test fold.

4) Repeat the process K times, so each fold is used once for testing.

5) Average the model's prediction error across all K folds

]

.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="888" style="display: block; margin: auto;" /&gt;

]

---

.pull-left45[

# K-Fold Cross-Validation

### Determining optimal Tuning parameters

By trying different tuning parameters in each iteration, you can determine which value minimises prediction error

Ex) Testing MAE values for values of `cp`



|Fold | cp = .05| cp = .10| &lt;high&gt;cp = .15&lt;/high&gt;| cp = .20|
|:----|--------:|--------:|--------:|--------:|
|1    |     5.13|     4.76|     4.24|     5.38|
|2    |     4.96|     4.54|     4.39|     5.72|
|3    |     5.34|     4.96|     4.13|     6.17|
|4    |     4.76|     5.13|     4.35|     5.20|
|Mean |     5.05|     4.85|     &lt;high&gt;4.28&lt;/high&gt;|     5.62|

Conclusion: `cp = .15` leads to the lowest test MAE

]

.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="888" style="display: block; margin: auto;" /&gt;

]

---

# K-Fold Cross-Validation

### Determining optimal Tuning parameters

Once the optimal value of a tuning parameter is determined through cross-validation, the algorithm is fit to the &lt;high&gt;entire training dataset&lt;/high&gt; using the &lt;high&gt;optimal tuning parameter&lt;/high&gt; resulting in the &lt;high&gt;Final Model&lt;/high&gt;

&lt;img src="image/training_final_optimisation.png" width="90%" style="display: block; margin: auto;" /&gt;



---
class: center,  middle

&lt;br&gt;&lt;br&gt;

# caret

## Cross-validation, and tuning parameter optimization

&lt;img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/Caret-package-in-R.png" width="60%" style="display: block; margin: auto;" /&gt;



---

# K-Fold Cross validation


.pull-left3[

Specify the use of k-fold cross-validation using the `trainControl()` function

- `method`: The resampling method, use `"cv"` for cross validation
- `number`: The number of folds

When you pass this object to `train()` (for any model), caret will find best parameters using cross-validation.

]

.pull-right65[


```r
# Specify 10 fold cross-validation
ctrl_cv &lt;- trainControl(method = "cv", 
                        number = 10) 

# Predict baselers income using lasso regression

lasso_mod &lt;- train(form = income ~ .,
                   data = baselers,
                   method = "glmnet",  # Penalised regression
                   trControl = ctrl_cv,
                   tuneGrid = expand.grid(alpha = 1, # Lasso
                                          lambda = 1:100))
```

]

---

# K-Fold Cross validation


.pull-left45[

If you plot your model object with `plot(XX_mod)` you will see a plot showing the relationship between the tuning parameter and error.

Print the best tuning parameter value with `XX_mod$bestTune$NAME`

]

.pull-right5[


```r
# Visualise tuning parameter error curve
plot(lasso_mod)
```

&lt;img src="Optimization_files/figure-html/unnamed-chunk-23-1.png" width="60%" style="display: block; margin: auto;" /&gt;

```r
# Print best tuning parameter values
lasso_mod$bestTune$lambda
```

```
[1] 28
```

]

---

# Regularised Regression

.pull-left35[

Your &lt;high&gt;final model&lt;/high&gt; is (as always) stored in `XX_mod$finalModel`. This is the model fit to the entire data using the &lt;high&gt;optimal tuning parameter(s)&lt;/high&gt;.

To get the coefficients from a Ridge or Lasso regression model, use the `coef()` function, with `XX_mod$finalModel` as the first argument, and the best tuning value as the second argument

In the output, values with . are coefficients that have been &lt;high&gt;removed with the lasso&lt;/high&gt;!

]

.pull-right6[


```r
# Print final model coefficients using best lambda
coef(lasso_mod$finalModel,      # Final Lasso model
     lasso_mod$bestTune$lambda) # Bets lambda value
```

```
25 x 1 sparse Matrix of class "dgCMatrix"
                                        1
(Intercept)                     498.79998
id                                .      
sexmale                           .      
age                             116.28163
height                            1.64721
weight                            .      
educationobligatory_school        .      
educationSEK_II                   .      
educationSEK_III                  .      
confessionconfessionless          0.52709
confessionevangelical-reformed    .      
confessionmuslim                  .      
confessionother                   .      
children                        -17.61816
happiness                      -125.28635
fitness                           .      
food                              2.29687
alcohol                          21.81538
tattoos                         -23.69171
rhine                             .      
datause                           .      
consultations                     7.87664
hiking                           -0.05348
fasnachtyes                     -21.07582
eyecoryes                         .      
```

]


---

# Estimating prediction accuracy

.pull-left35[

If you have fit many models with cross-validation, you can compare their estimated prediction performance with `resamples()`

The main argument to `resamples()` should be a list of all of your models created with `train()`

If you print the `summary()` of this object, it will print 'prediction' error statistics from cross-validation during training. This is your estimate of future prediction performance!

]

.pull-right6[


```r
# Get accuracy statistics across folds for three
#  models that I have fit with cross validation

resamples_mod &lt;- resamples(list(ridge = ridge_mod, 
                                lasso = lasso_mod, 
                                glm = glm_mod))

# Print summary of accuracies
summary(resamples_mod)

# MAE 
#         Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
# ridge 1057.9  1078.9 1083.2 1090.1  1112.0 1118.5    0
# lasso  865.7   892.7  897.2  902.9   914.2  944.8    0
# glm    839.7   910.0  920.5  905.9   922.7  936.7    0
```

]

---
class: middle, center

&lt;h1&gt; Questions?&lt;/h1&gt;


&lt;h1&gt;&lt;a href=https://therbootcamp.github.io/appliedML_2019Jan/_sessions/Optimization/Optimization_practical.html&gt;Practical&lt;/a&gt;&lt;/h1&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
