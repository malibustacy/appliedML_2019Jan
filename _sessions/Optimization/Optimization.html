<!DOCTYPE html>
<html>
  <head>
    <title>Optimization</title>
    <meta charset="utf-8">
    <meta name="author" content="Applied Machine Learning with R www.therbootcamp.com @therbootcamp" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="baselrbootcamp.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Optimization
## Regularization and Cross-Validation
### Applied Machine Learning with R<br/><a href='https://therbootcamp.github.io'>www.therbootcamp.com</a><br/><a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### January 2019

---


layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;Applied Machine Learning with R, January 2019&lt;/font&gt;&lt;/a&gt;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&lt;a href="https://therbootcamp.github.io/"&gt;&lt;font color="#7E7E7E"&gt;www.therbootcamp.com&lt;/font&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/div&gt;

---










.pull-left4[
&lt;br&gt;&lt;br&gt;
# Where we are

- &lt;high&gt;Train&lt;/high&gt; one of several models (&lt;high&gt;regression&lt;/high&gt;, &lt;high&gt;decision trees&lt;/high&gt;, and &lt;high&gt;random forests&lt;/high&gt;) on training data.

- Explore models - show regression coefficients, plot decision trees (etc)

- Assess model &lt;high&gt;prediction&lt;/high&gt; performance on &lt;high&gt;test&lt;/high&gt; data
    - Mean Absolute Error (MAE)

]

.pull-right55[

### Model Training

&lt;img src="image/model_training_flow.png" width="1442" style="display: block; margin: auto;" /&gt;

### Model Testing

&lt;img src="image/model_testing_flow.png" width="1439" style="display: block; margin: auto;" /&gt;

]

---


.pull-left4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;
# Overfitting

When a model is consistently &lt;high&gt;less accurate in predicting future data&lt;/high&gt; than in &lt;high&gt;fitting training data&lt;high&gt;, this is called &lt;high&gt;overfitting&lt;/high&gt;

Just because model A is better than model B in training, does not mean it will be better in testing!

Extremely flexible models that tend to overfit are like 'wolves in sheep's clothing'

]


.pull-right55[


&lt;br&gt;&lt;br&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/wolf_complex.png" alt="&amp;lt;font size = 4&amp;gt;victoriarollison.com (adapted)&amp;lt;/font&amp;gt;" width="100%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;victoriarollison.com (adapted)&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]

---

.pull-left4[
&lt;br&gt;&lt;br&gt;&lt;br&gt;

# Overfitting

### How will we try to avoid overfitting?

Use regression models with &lt;high&gt;regularization&lt;/high&gt; terms, such as &lt;high&gt;ridge&lt;/high&gt; and &lt;high&gt;lasso&lt;/high&gt; which explicitly &lt;high&gt;punish model complexity&lt;/high&gt;.

Use &lt;high&gt;cross-validation&lt;/high&gt; to find &lt;high&gt;optimal tuning parameters&lt;/high&gt;, including regularization.

]

.pull-right55[

### Regularized Regression

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/lasso_penalty_eq.jpg" alt="L1 Lasso Penalty" width="100%" /&gt;
&lt;p class="caption"&gt;L1 Lasso Penalty&lt;/p&gt;
&lt;/div&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---

.pull-left55[

# Tuning parameters (Recap)

### What are tuning parameters?

&lt;high&gt;Tuning parameters&lt;/high&gt; are parameters that &lt;high&gt;guide&lt;/high&gt; (aka. 'tune') a model during fitting.


- Decision trees: complexity tuning parameter &lt;high&gt;cp&lt;/high&gt;

- Random forests diversity tuning parameter &lt;high&gt;mtry&lt;/high&gt;

Tuning parameters do not show up in the final model (you never see a complexity parameter in a final decision tree)! They are only used to guide fitting.

There is not one 'best' tuning parameter, it always depends on your specific dataset.

]

.pull-right45[

&lt;img src="image/complexity_parameter.png" width="80%" style="display: block; margin: auto;" /&gt;

&lt;img src="image/mtry_parameter.png" width="80%" style="display: block; margin: auto;" /&gt;

]


---

# Regularized Regression

There are two common methods to fit penalized (aka regularized) regression models: Ridge and Lasso. Each penalizes regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left5[

### Ridge

The Ridge penalty is known as the `\(\ell2\)` norm, where Beta weights are selected by &lt;high&gt;minimizing&lt;/high&gt; the following equation:

&lt;img src="image/ridge_penalty_eq.jpg" width="100%" style="display: block; margin: auto;" /&gt;

As `\(\lambda\)` "Lambda" increases, coefficients are pushed towards (but not necessarily exactly to) 0.

See [Wikipedia's Ridge article](https://en.wikipedia.org/wiki/Tikhonov_regularization) to learn more.

]

.pull-right45[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/ridge_penalty_plot.png" alt="&amp;lt;font size = 4&amp;gt;James et al., ISLR&amp;lt;/font&amp;gt;" width="80%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;James et al., ISLR&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]

---

# Regularized Regression

There are two common methods to fit penalized (aka regularized) regression models: Ridge and Lasso. Each penalizes regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left3[

### Ridge

To fit Ridge penalized regression in R, use `method = "glmnet"`.

In the `tuneGrid` argument:

- `alpha = 0` indicates the `\(\ell2\)` Ridge penalty.
- `lambda` = Vector of lambda tuning parameters values to try.

]

.pull-right65[
&lt;br&gt;

```r
# Train ridge penalised regression model in R

train(form = criterion ~ .,
      data = data_train,
      method = "glmnet",
      trControl = ctrl,
      preProcess = c("center", "scale"),  # Standardise
      tuneGrid = expand.grid(alpha = 0, # Ridge penalty
                              lambda = 1:100)) # Lambda
```

]

---

# Regularized Regression

There are two common methods to fit penalized (aka regularized) regression models: Ridge and Lasso. Each penalizes regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left5[

### Lasso

The Lasso penalty is known as the `\(\ell1\)` norm, where Beta weights are selected by minimizing the following equation:

&lt;img src="image/lasso_penalty_eq.jpg" width="100%" style="display: block; margin: auto;" /&gt;

As `\(\lambda\)` increases, coefficients are pushed towards 0, with some being forced to &lt;high&gt;exactly 0&lt;/high&gt;.

See [Wikipedia's Lasso article](https://en.wikipedia.org/wiki/Lasso_(statistics&amp;#x29;) to learn more.

]

.pull-right45[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="image/lasso_penalty_plot.png" alt="&amp;lt;font size = 4&amp;gt;James et al., ISLR&amp;lt;/font&amp;gt;" width="80%" /&gt;
&lt;p class="caption"&gt;&lt;font size = 4&gt;James et al., ISLR&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;

]

---

# Regularized Regression

There are two common methods to fit penalized (aka regularized) regression models: Ridge and Lasso. Each penalizes regression models for having large `\(\beta\)` values using the &lt;high&gt;Lambda tuning parameter&lt;/high&gt;

.pull-left3[

### Lasso

To fit Lasso penalized regression in R, use `method = "glmnet"`.

In the `tuneGrid` argument:

- `alpha = 1` indicates the `\(\ell1\)` Lasso penalty
- `lambda` = Vector of lambda tuning parameters values to try.

]

.pull-right65[
&lt;br&gt;

```r
# Train Lasso penalised regression model in R

train(form = criterion ~ .,
      data = data_train,
      method = "glmnet",
      trControl = ctrl,
      preProcess = c("center", "scale"),  # Standardise
      tuneGrid = expand.grid(alpha = 1, # Lasso penalty
                              lambda = 1:100)) # Lambda
```

]


---

.pull-left45[

# K-Fold Cross-Validation

### What is it?

Cross-validation is a sampling procedure performed on training data used to &lt;high&gt;estimate a model's prediction performance&lt;/high&gt; in future test data, and to determine &lt;high&gt;optimal tuning parameters&lt;/high&gt; selected to minimize prediction error.

Cross-validation is not "cheating": because it is only performed on the training data (never on the true test dataset)

After cross-validation is complete, the model is trained on the entire dataset, using optimal tuning parameters, resulting in a &lt;high&gt;final model&lt;/high&gt; which can be used for future model testing.

]

.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="888" style="display: block; margin: auto;" /&gt;

]

---

.pull-left45[

# K-Fold Cross-Validation

### Steps

1) Split the original training data into K 'folds' (mutually exclusive groups of cases)

2) Select K - 1 folds for training, and 1 fold for testing.

3) Fit the model to the K - 1 training folds, and evaluate its testing accuracy on the test fold.

4) Repeat the process K times, so each fold is used once for testing.

5) Average the model's prediction error across all K folds

]

.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="888" style="display: block; margin: auto;" /&gt;

]

---

.pull-left45[

# K-Fold Cross-Validation

### Determining optimal Tuning parameters

By trying different tuning parameters in each iteration, you can determine which value minimizes prediction error

Ex) Testing MAE values for values of `cp`



|Fold | cp = .05| cp = .10| &lt;high&gt;cp = .15&lt;/high&gt;| cp = .20|
|:----|--------:|--------:|--------:|--------:|
|1    |     5.13|     4.76|     4.24|     5.38|
|2    |     4.96|     4.54|     4.39|     5.72|
|3    |     5.34|     4.96|     4.13|     6.17|
|4    |     4.76|     5.13|     4.35|     5.20|
|Mean |     5.05|     4.85|     &lt;high&gt;4.28&lt;/high&gt;|     5.62|

Conclusion: `cp = .15` leads to the lowest test MAE

]

.pull-right5[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### Cross Validation

&lt;img src="image/crossvalidation_4fold.png" width="888" style="display: block; margin: auto;" /&gt;

]

---

# K-Fold Cross-Validation

### Determining optimal Tuning parameters

Once the optimal value of a tuning parameter is determined through cross-validation, the algorithm is fit to the &lt;high&gt;entire training dataset&lt;/high&gt; using the &lt;high&gt;optimal tuning parameter&lt;/high&gt; resulting in the &lt;high&gt;Final Model&lt;/high&gt;

&lt;img src="image/training_final_optimisation.png" width="90%" style="display: block; margin: auto;" /&gt;



---
class: center,  middle

&lt;br&gt;&lt;br&gt;

# caret

## Cross-validation, and tuning parameter optimization

&lt;img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/Caret-package-in-R.png" width="60%" style="display: block; margin: auto;" /&gt;



---

# K-Fold Cross validation


.pull-left3[

Specify the use of k-fold cross-validation using the `trainControl()` function

- `method`: The resampling method, use `"cv"` for cross validation
- `number`: The number of folds

When you pass this object to `train()` (for any model), caret will find best parameters using cross-validation.

]

.pull-right65[


```r
# Specify 10 fold cross-validation
ctrl_cv &lt;- trainControl(method = "cv",
                        number = 10)

# Predict baselers income using lasso regression

lasso_mod &lt;- train(form = income ~ .,
                   data = baselers,
                   method = "glmnet",  # Penalised regression
                   trControl = ctrl_cv,
                   preProcess = c("center", "scale"),  # Standardise
                   tuneGrid = expand.grid(alpha = 1, # Lasso
                                          lambda = 1:100))
```

]


---

# K-Fold Cross validation

.pull-left45[

If you print model object `XX_mod` you will see summary statistics showing how the model performed on averge in test folds for different values of the tuning parameters


At the bottom of the output, you'll see a summary message telling you how the best tuning parameter was found.

`Tuning parameter 'alpha' was held constant at a value of 1.`

`RMSE was used to select the optimal model using the`
` smallest value.`

`The final values used for the model were alpha = 1 and`
` lambda = 28.`

]

.pull-right5[


```r
# Print summary information
lasso_mod
```

```
glmnet 

1000 samples
  19 predictor

Pre-processing: centered (24), scaled (24) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 900, 901, 900, 901, 900, 899, ... 
Resampling results across tuning parameters:

  lambda  RMSE  Rsquared  MAE  
    1     1029  0.8631    811.1
    2     1029  0.8631    811.1
    3     1029  0.8631    811.0
    4     1028  0.8633    810.5
    5     1028  0.8634    810.0
    6     1027  0.8636    809.6
    7     1026  0.8637    809.2
    8     1026  0.8638    808.8
    9     1026  0.8640    808.5
   10     1025  0.8641    808.2
   11     1025  0.8642    808.0
   12     1025  0.8643    807.8
   13     1024  0.8643    807.6
   14     1024  0.8644    807.4
   15     1024  0.8645    807.1
   16     1023  0.8646    807.0
   17     1023  0.8647    806.8
   18     1023  0.8647    806.7
   19     1023  0.8648    806.6
   20     1023  0.8649    806.5
   21     1023  0.8649    806.5
   22     1022  0.8649    806.4
   23     1022  0.8650    806.3
   24     1022  0.8650    806.2
   25     1022  0.8650    806.1
   26     1022  0.8651    806.0
   27     1022  0.8651    806.0
   28     1022  0.8651    806.0
   29     1022  0.8651    806.0
   30     1022  0.8651    806.0
   31     1022  0.8651    806.0
   32     1022  0.8651    806.1
   33     1022  0.8651    806.2
   34     1022  0.8651    806.3
   35     1023  0.8651    806.4
   36     1023  0.8651    806.5
   37     1023  0.8651    806.7
   38     1023  0.8651    806.9
   39     1023  0.8650    807.1
   40     1023  0.8650    807.3
   41     1023  0.8650    807.5
   42     1024  0.8650    807.7
   43     1024  0.8649    807.9
   44     1024  0.8649    808.2
   45     1024  0.8649    808.4
   46     1024  0.8648    808.6
   47     1025  0.8648    808.9
   48     1025  0.8648    809.2
   49     1025  0.8647    809.4
   50     1025  0.8647    809.7
   51     1026  0.8646    810.0
   52     1026  0.8646    810.3
   53     1026  0.8645    810.6
   54     1026  0.8645    811.0
   55     1027  0.8644    811.3
   56     1027  0.8644    811.6
   57     1027  0.8643    811.9
   58     1028  0.8642    812.3
   59     1028  0.8642    812.6
   60     1028  0.8641    813.0
   61     1029  0.8640    813.3
   62     1029  0.8639    813.7
   63     1029  0.8639    814.1
   64     1030  0.8638    814.5
   65     1030  0.8637    814.9
   66     1031  0.8636    815.2
   67     1031  0.8635    815.7
   68     1032  0.8634    816.1
   69     1032  0.8633    816.5
   70     1032  0.8632    817.0
   71     1033  0.8632    817.4
   72     1033  0.8631    817.8
   73     1034  0.8630    818.3
   74     1034  0.8629    818.7
   75     1035  0.8628    819.2
   76     1035  0.8627    819.6
   77     1036  0.8626    820.1
   78     1036  0.8625    820.6
   79     1037  0.8624    821.1
   80     1037  0.8622    821.6
   81     1038  0.8621    822.1
   82     1038  0.8620    822.6
   83     1039  0.8619    823.1
   84     1039  0.8618    823.6
   85     1040  0.8617    824.1
   86     1040  0.8616    824.6
   87     1041  0.8615    825.1
   88     1041  0.8614    825.6
   89     1042  0.8613    826.1
   90     1042  0.8612    826.6
   91     1043  0.8611    827.1
   92     1043  0.8610    827.5
   93     1044  0.8609    828.0
   94     1044  0.8607    828.5
   95     1045  0.8606    829.0
   96     1045  0.8605    829.5
   97     1046  0.8604    830.0
   98     1046  0.8603    830.5
   99     1047  0.8602    831.1
  100     1048  0.8601    831.6

Tuning parameter 'alpha' was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 1 and lambda = 28.
```

]

---

# K-Fold Cross validation

.pull-left45[

If you plot your model object with `plot(XX_mod)` you will see a plot showing the relationship between the tuning parameter and error.

The bottom of the curve shows the best tuning parameter (minimises error)

Print the best tuning parameter value with `XX_mod$bestTune$NAME`

]

.pull-right5[


```r
# Visualise tuning parameter error curve
plot(lasso_mod)
```

&lt;img src="Optimization_files/figure-html/unnamed-chunk-24-1.png" width="60%" style="display: block; margin: auto;" /&gt;

```r
# Print best tuning parameter values
lasso_mod$bestTune$lambda
```

```
[1] 28
```

]

---

# Regularized Regression

.pull-left35[

Your &lt;high&gt;final model&lt;/high&gt; is (as always) stored in `XX_mod$finalModel`. This is the model fit to the entire data using the &lt;high&gt;optimal tuning parameter(s)&lt;/high&gt;.

To get the coefficients from a Ridge or Lasso regression model, use the `coef()` function, with `XX_mod$finalModel` as the first argument, and the best tuning value as the second argument

In the output, values with . are coefficients that have been &lt;high&gt;removed with the lasso&lt;/high&gt;!

]

.pull-right6[


```r
# Print final model coefficients using best lambda
coef(lasso_mod$finalModel,      # Final Lasso model
     lasso_mod$bestTune$lambda) # Best lambda value
```

```
25 x 1 sparse Matrix of class "dgCMatrix"
                                       1
(Intercept)                    7569.6832
id                                .     
sexmale                           .     
age                            1925.9098
height                           21.6221
weight                            .     
educationobligatory_school        .     
educationSEK_II                   .     
educationSEK_III                  .     
confessionconfessionless          0.2419
confessionevangelical-reformed    .     
confessionmuslim                  .     
confessionother                   .     
children                        -18.0667
happiness                      -132.5714
fitness                           .     
food                            893.9188
alcohol                         388.3610
tattoos                         -61.3159
rhine                             .     
datause                           .     
consultations                    14.7233
hiking                           -1.1943
fasnachtyes                      -3.9282
eyecoryes                         .     
```

]


---

# Estimating prediction accuracy

.pull-left35[

If you have fit many models with cross-validation, you can compare their estimated prediction performance with `resamples()`

The main argument to `resamples()` should be a list of all of your models created with `train()`

If you print the `summary()` of this object, it will print 'prediction' error statistics from cross-validation during training. This is your estimate of future prediction performance!

]

.pull-right6[


```r
# Get accuracy statistics across folds for three
#  models that I have fit with cross validation

resamples_mod &lt;- resamples(list(ridge = ridge_mod,
                                lasso = lasso_mod,
                                glm = glm_mod))

# Print summary of accuracies
summary(resamples_mod)

# MAE
#         Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
# ridge 1057.9  1078.9 1083.2 1090.1  1112.0 1118.5    0
# lasso  865.7   892.7  897.2  902.9   914.2  944.8    0
# glm    839.7   910.0  920.5  905.9   922.7  936.7    0
```

]

---
class: middle, center

&lt;h1&gt; Questions?&lt;/h1&gt;


&lt;h1&gt;&lt;a href=https://therbootcamp.github.io/appliedML_2019Jan/_sessions/Optimization/Optimization_practical.html&gt;Practical&lt;/a&gt;&lt;/h1&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
